{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Introduction to Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal\n",
    "\n",
    "In this notebook, we will explore **Ensemble Learning** including:\n",
    "* Bagging\n",
    "* Random Forest\n",
    "* AdaBoost\n",
    "\n",
    "For the **Decision Tree** method, you will:\n",
    "* Use open-source package to do ensemble learning\n",
    "* Compare performances of different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bias-Variance Decomposition\n",
    "\n",
    "\\begin{align}\n",
    "E[(y-\\hat{f}(x))^2] &= (E[\\hat{f}(x)-f(x)])^2 + E[\\hat{f}(x)-E[\\hat{f}(x)]]^2 + \\sigma^2  \\\\\n",
    "&= \\text{Bias}[\\hat{f}(x)]^2 + \\text{Var}[\\hat{f}(x)] + \\text{Irreducibel Error}\n",
    "\\end{align}\n",
    "\n",
    "A figure to illustrate **Bias** and **Variance**:\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/bias-and-variance.jpg\" width=\"400\">\n",
    "\n",
    "A chart to understand the tradeoff of **Bias** and **Variance**:\n",
    "<img src=\"http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png\" width=\"500\">\n",
    "\n",
    "An example to show **Overfit** problem: <br/>\n",
    "Which line is more preferred? Black or Green?\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/320px-Overfitting.svg.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Ensemble Learning\n",
    "Ensemble learning methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone ([Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning)). \n",
    "\n",
    "Ensemble learning is an appropriate way to somehow \"relieve\" overfitting problem (Remember that ensemble learning methods may still overfit). From the lecture class, we know that a basic workflow of ensemble method:\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Nipaporn_Chanamarn/publication/308368870/figure/fig4/AS:408666126733315@1474445005322/The-concept-diagram-of-stacking-ensemble-learning-32.jpg \"Ensemble Learning\")\n",
    "\n",
    "Examples of Ensemble Learning methods include:\n",
    "<ol>\n",
    "    <li>Bootstrap Aggregating (aka.Bagging)</li>\n",
    "    <li>Random Forest</li>\n",
    "    <li>Boosting</li>    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Examples\n",
    "\n",
    "### 3.1 Case on Handwritten Digit Recognition\n",
    "\n",
    "#### Dataset:\n",
    "\n",
    "The **Kaggle** competition dataset can be obtained from https://www.kaggle.com/c/digit-recognizer/data. \n",
    "\n",
    "#### Overview:\n",
    "\n",
    "MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of **handwritten images** has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "\n",
    "In this competition, your goal is to **correctly identify digits** from a dataset of tens of thousands of handwritten images. We’ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.\n",
    "\n",
    "#### Acknowlegements:\n",
    "\n",
    "More details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at http://yann.lecun.com/exdb/mnist/index.html. The dataset is made available under a Creative Commons Attribution-Share Alike 3.0 license.\n",
    "\n",
    "#### Attributes:\n",
    "\n",
    "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "\n",
    "The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n",
    "\n",
    "The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset: You need to download dataset first\n",
    "%pwd\n",
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42,000 pictures; Each picture is composed of 28*28 dimensional pixels\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does an image look like\n",
    "plt.imshow(np.array(train.iloc[1,1:]).reshape((28, 28)), cmap=\"gray\")\n",
    "plt.title(\"This digit is %d\" % train.iloc[1,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bagging\n",
    "\n",
    "A basic procedure for **Bagging** is:\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/python-deeper-insights/9781787128576/graphics/3547_07_06.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Aggregating Package\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values\n",
    "\n",
    "# Split train data: 70% for model fit, 30% for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_feature, train_target, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Bagging Model; A bundle of decision trees\n",
    "# Just wait a minute...It will take some time\n",
    "BA = BaggingClassifier(n_estimators=100, random_state=12345)\n",
    "BA_model = BA.fit(X_train, y_train)\n",
    "BA_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "y_pred_valid = BA_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Random Forest model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_valid)\n",
    "print cm\n",
    "print \"Accuracy: \", accuracy_score(y_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest\n",
    "\n",
    "A basic procedure for **Random Forest** is:\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/ajTc5y3OqSQ/hqdefault.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest package\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values\n",
    "\n",
    "# Split train data: 70% for model fit, 30% for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_feature, train_target, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Random Forest Model; Binary Splitting using Entropy\n",
    "RF = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=12345)\n",
    "RF_model = RF.fit(X_train, y_train)\n",
    "RF_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "y_pred_valid = RF_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Random Forest model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_valid)\n",
    "print cm\n",
    "print \"Accuracy: \", accuracy_score(y_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical procedure for boosting method is:\n",
    "\n",
    "<img src=\"https://koalaverse.github.io/machine-learning-in-R/images/boosting_diagram.png\" width=\"500\">\n",
    "\n",
    "Examples of boosting methods include:\n",
    "1. Boosting based on weights: Adaptive boosting (Adaboost)\n",
    "2. Boosting based on residuals: Gradient boosting decision trees (GBDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for **Adaboost** is:\n",
    "\n",
    "<img src=\"https://koalaverse.github.io/machine-learning-in-R/images/adaboost_m1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between err and alpha\n",
    "err = np.linspace(0, 1, num=100)\n",
    "alpha = map(lambda x: np.log((1-x)/x), err)\n",
    "\n",
    "# Plot the relationship\n",
    "plt.scatter(err, alpha, color='blue')\n",
    "plt.xlabel('err')\n",
    "plt.ylabel('alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost package\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values\n",
    "\n",
    "# Split train data: 70% for model fit, 30% for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_feature, train_target, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Adaboosting Model\n",
    "Ada = AdaBoostClassifier(n_estimators=100, random_state=12345)\n",
    "Ada_model = Ada.fit(X_train, y_train)\n",
    "Ada_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "y_pred_valid = Ada_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Adaboosting model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_valid)\n",
    "print cm\n",
    "print \"Accuracy: \", accuracy_score(y_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about Ensemble Learning can be found at http://scikit-learn.org/stable/modules/ensemble.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose: Comparing single decision tree model with ensemble methods (5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Fold Cross Validation\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a single decision tree model**\n",
    "* Set `criterion='entropy'`, and `random_state=12345`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Single decision tree model\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy', random_state=12345)\n",
    "\n",
    "# Cross validation on Model 1\n",
    "cv_model_1 = cross_val_score(decision_tree, # Cross-validation on Model 1\n",
    "                             train.iloc[:,1:], # Feature matrix\n",
    "                             train.iloc[:,0], # Output vector\n",
    "                             cv=kf, # Cross-validation technique\n",
    "                             scoring='accuracy' # Model performance metrics: accuracy\n",
    "                            )\n",
    "\n",
    "# Report performance of Model 1\n",
    "print \"Single decision tree model: %s\" %(cv_model_1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a bagging model**\n",
    "* Set `n_estimators=100`, and `random_state=12345`\n",
    "* Reference: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Bagging method\n",
    "bagging = BaggingClassifier(n_estimators=100, random_state=12345)\n",
    "\n",
    "# Cross validation on Model 2\n",
    "cv_model_2 = cross_val_score(bagging, # Cross-validation on Model 2\n",
    "                             train.iloc[:,1:], # Feature matrix\n",
    "                             train.iloc[:,0], # Output vector\n",
    "                             cv=kf, # Cross-validation technique\n",
    "                             scoring='accuracy' # Model performance metrics: accuracy\n",
    "                            )\n",
    "\n",
    "# Report performance of Model 2\n",
    "print \"Bagging model: %s\" %(cv_model_2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1. Create a random forest model (2 points)**\n",
    "* Set `criterion='entropy'`, `n_estimators=100`, and `random_state=12345`\n",
    "* Reference: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Random Forest method\n",
    "random_forest = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=12345)\n",
    "\n",
    "# Cross validation on Model 3\n",
    "cv_model_3 = cross_val_score(random_forest, # Cross-validation on Model 3\n",
    "                             train.iloc[:,1:], # Feature matrix\n",
    "                             train.iloc[:,0], # Output vector\n",
    "                             cv=kf, # Cross-validation technique\n",
    "                             scoring='accuracy' # Model performance metrics: accuracy\n",
    "                            )\n",
    "\n",
    "# Report performance of Model 3\n",
    "print \"Bagging model: %s\" %(cv_model_3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2. Create an Adaboost model (2 points)**\n",
    "* Set `n_estimators=100`, and `random_state=12345`\n",
    "* Reference: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: AdaBoosting method\n",
    "Adaboost = AdaBoostClassifier(n_estimators=100, random_state=12345)\n",
    "\n",
    "# Cross validation on Model 4\n",
    "cv_model_4 = cross_val_score(Adaboost, # Cross-validation on Model 4\n",
    "                             train.iloc[:,1:], # Feature matrix\n",
    "                             train.iloc[:,0], # Output vector\n",
    "                             cv=kf, # Cross-validation technique\n",
    "                             scoring='accuracy' # Model performance metrics: accuracy\n",
    "                            )\n",
    "\n",
    "# Report performance of Model 4\n",
    "print \"Bagging model: %s\" %(cv_model_4.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3. Based on your results, what is your finding? (1 point)**\n",
    "\n",
    "Hint: Just write your findings based on your model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 References\n",
    "\n",
    "[1] Chris Albon. (2018). Machine Learning with Python Cookbook. O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
