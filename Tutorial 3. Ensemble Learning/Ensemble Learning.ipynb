{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Introduction to Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal\n",
    "\n",
    "In this notebook, we will explore **Ensemble Learning** including:\n",
    "* Bagging\n",
    "* Random Forest\n",
    "* AdaBoost\n",
    "* Gradient Boost\n",
    "\n",
    "For the **Decision Tree** method, you will:\n",
    "* Use open-source package to do ensemble learning\n",
    "* Compare performances of different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bias-Variance Decomposition\n",
    "\n",
    "\\begin{align}\n",
    "E[(y-\\hat{f}(x))^2] &= (E[\\hat{f}(x)-f(x)])^2 + E[\\hat{f}(x)-E[\\hat{f}(x)]]^2 + \\sigma^2  \\\\\n",
    "&= \\text{Bias}[\\hat{f}(x)]^2 + \\text{Var}[\\hat{f}(x)] + \\text{Irreducibel Error}\n",
    "\\end{align}\n",
    "\n",
    "A figure to illustrate **Bias** and **Variance**:\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/bias-and-variance.jpg\" width=\"400\">\n",
    "\n",
    "A chart to understand the tradeoff of **Bias** and **Variance**:\n",
    "<img src=\"http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png\" width=\"500\">\n",
    "\n",
    "An example to show **Overfit** problem: <br/>\n",
    "Which line is more preferred? Black or Green?\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/320px-Overfitting.svg.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: <br/>\n",
    "In previous lectures, you have already learned several techniques to overcome overfitting problem. What are [they](https://elitedatascience.com/overfitting-in-machine-learning)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Ensemble Learning\n",
    "Ensemble learning methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone ([Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning)). \n",
    "\n",
    "Ensemble learning is an appropriate way to somehow \"relieve\" overfitting problem (Remember that ensemble learning methods may still overfit). From the lecture class, we know that a basic workflow of ensemble method:\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Nipaporn_Chanamarn/publication/308368870/figure/fig4/AS:408666126733315@1474445005322/The-concept-diagram-of-stacking-ensemble-learning-32.jpg \"Ensemble Learning\")\n",
    "\n",
    "Examples of Ensemble Learning methods include:\n",
    "<ol>\n",
    "    <li>Bootstrap Aggregating (aka.Bagging)</li>\n",
    "    <li>Random Forest</li>\n",
    "    <li>Boosting</li>    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Examples\n",
    "\n",
    "### 3.1 Case on Handwritten Digit Recognition\n",
    "\n",
    "#### Dataset:\n",
    "\n",
    "The **Kaggle** competition dataset can be obtained from https://www.kaggle.com/c/digit-recognizer/data. \n",
    "\n",
    "#### Overview:\n",
    "\n",
    "MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of **handwritten images** has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "\n",
    "In this competition, your goal is to **correctly identify digits** from a dataset of tens of thousands of handwritten images. We’ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.\n",
    "\n",
    "#### Acknowlegements:\n",
    "\n",
    "More details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at http://yann.lecun.com/exdb/mnist/index.html. The dataset is made available under a Creative Commons Attribution-Share Alike 3.0 license.\n",
    "\n",
    "#### Attributes:\n",
    "\n",
    "The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "\n",
    "The test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n",
    "\n",
    "The evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset: You need to download dataset first\n",
    "%pwd\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42,000 pictures; Each picture is composed of 28*28 dimensional pixels\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does an image look like\n",
    "plt.imshow(np.array(train.iloc[1,1:]).reshape((28, 28)), cmap=\"gray\")\n",
    "plt.title(\"This digit is %d\" % train.iloc[1,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bagging\n",
    "\n",
    "A basic procedure for **Bagging** is:\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/python-deeper-insights/9781787128576/graphics/3547_07_06.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Aggregating Package\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values\n",
    "test_feature = test.iloc[:,1:].values\n",
    "test_target = test.iloc[:,0].values\n",
    "\n",
    "# Split train data: 70% for model fit, 30% for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_feature, train_target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Bagging Model; Binary Splitting using Entropy\n",
    "# Just wait a minute...It will take some time\n",
    "BA = BaggingClassifier(n_estimators=100, random_state=0)\n",
    "BA_model = BA.fit(X_train, y_train)\n",
    "BA_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "y_pred_valid = BA_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Random Forest model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_valid)\n",
    "print cm\n",
    "print \"Accuracy: \", accuracy_score(y_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy is 94.99%, which is quite ok. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest\n",
    "\n",
    "A basic procedure for **Random Forest** is:\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/ajTc5y3OqSQ/hqdefault.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest package\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Tranform dataframe to array\n",
    "train_feature = train.iloc[:,1:].values\n",
    "train_target = train.iloc[:,0].values\n",
    "test_feature = test.iloc[:,1:].values\n",
    "test_target = test.iloc[:,0].values\n",
    "\n",
    "# Split train data: 70% for model fit, 30% for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_feature, train_target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Random Forest Model; Binary Splitting using Entropy\n",
    "RF = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=0)\n",
    "RF_model = RF.fit(X_train, y_train)\n",
    "RF_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "y_pred_valid = RF_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of Random Forest model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_valid)\n",
    "print cm\n",
    "print \"Accuracy: \", accuracy_score(y_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy is 95.95%, even better. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical procedure for boosting method is:\n",
    "\n",
    "<img src=\"https://koalaverse.github.io/machine-learning-in-R/images/boosting_diagram.png\" width=\"500\">\n",
    "\n",
    "Examples of boosting methods include:\n",
    "1. Boosting based on weights: Adaptive boosting (Adaboost)\n",
    "2. Boosting based on residuals: Gradient boosting decision trees (GBDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for **Adaboost** is:\n",
    "\n",
    "<img src=\"https://koalaverse.github.io/machine-learning-in-R/images/adaboost_m1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris() \n",
    "\n",
    "# Train Adaboost Model with 100 subclassifiers\n",
    "Ada = AdaBoostClassifier(n_estimators=100) \n",
    "\n",
    "# Cross validation\n",
    "scores = cross_val_score(Ada, iris.data, iris.target, cv=10) \n",
    "scores.mean()                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average cross-validation accuracy is 94.67%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for **Gradient Boosting** is:\n",
    "\n",
    "<img src=\"https://koalaverse.github.io/machine-learning-in-R/images/friedman_gbm.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris() \n",
    "\n",
    "# Train Gradient Boosting Model with 100 subclassifiers\n",
    "GB = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "# Cross validation\n",
    "scores = cross_val_score(GB, iris.data, iris.target, cv=10) \n",
    "scores.mean()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average cross-validation accuracy is 95.33%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about Ensemble Learning can be found at http://scikit-learn.org/stable/modules/ensemble.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 References\n",
    "\n",
    "[1] Chris Albon. (2018). Machine Learning with Python Cookbook. O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
