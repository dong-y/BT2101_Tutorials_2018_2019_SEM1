{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Introduction to Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal:\n",
    "\n",
    "In this notebook, we will explore multiple linear regression using:\n",
    "* Gradient descent method\n",
    "* Open-source package: `scikit-learn`\n",
    "\n",
    "For the gradient descent method, you will:\n",
    "* Use numpy to write functions\n",
    "* Write a derivative function\n",
    "* Write an output function\n",
    "* Write a gradient descent function\n",
    "* Add a constant column of 1's as intercept term\n",
    "* Use the gradient descent function to get regression estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Summary of Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture class, we know that a typical linear regression model of *N* observations and *p* predictors:\n",
    "\n",
    "\\begin{align}\n",
    "Y &= \\beta_0 + \\sum_{j=1}^p x_j \\beta_j + \\varepsilon \\\\\n",
    "&= X\\beta + \\varepsilon \n",
    "\\end{align}\n",
    "\n",
    "And we aim to **Minimize** the loss function:\n",
    "\n",
    "$$RSS(\\beta) = \\varepsilon^T\\varepsilon = \\sum_{i=1}^{N}(y_i - \\beta_0 - \\sum_{j=1}^p x_j \\beta_j)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the predicted output is calculated by the dot product of features and weights.<br/>\n",
    "Suppose we have: <br/>\n",
    "Two features: $x_i = [2.0, 1.5]$ <br/>\n",
    "and Weights: $\\beta = [1.0, 1.0]$ <br/>\n",
    "Then the predicted output: $ \\hat{y}_i = 1.0\\times2.0 + 1.0\\times1.5 = 3.5 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(feature_matrix, weight_vector):\n",
    "    '''This function is used to predict outputs.\n",
    "    \n",
    "    Inputs:\n",
    "    1) feature_matrix: A matrix of selected features;\n",
    "    2) weight_vector: A vector of weights for selected features;\n",
    "    \n",
    "    Outputs:\n",
    "    1) predict_vector: A vector of predicted outputs   \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    prediction_vector = np.dot(feature_matrix, weight_vector)\n",
    "\n",
    "    return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "features = np.array([[2.0, 1.5], [4.8, 5.2]])\n",
    "weights = np.ones((2, 1))\n",
    "predict(features, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Gradient Descent\n",
    "### 2.1 Computing Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we compute the derivatives of the loss function? <br/>\n",
    "The loss function is calculated by summing up the squared errors over *N* data observations/rows:\n",
    "$$RSS(\\beta) = \\sum_{i=1}^{N}(y_i - \\beta_0 - \\sum_{j=1}^p x_j \\beta_j)^2 $$\n",
    "\n",
    "Just doing simple math, you will find that the derivative of a sum equals to the sum of the derivatives. For a single observation *i*, the derivative with respect to feature $x_j$ for this observation *i* is $ -2\\varepsilon_{i}x_{ij} $. So the derivative with respect to feature $x_j$ is $ -2\\sum_{i=1}^N \\varepsilon_{i}x_{ij} = -2\\varepsilon^{T}x_j $. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(feature_matrix, error_vector):\n",
    "    '''This function is used to calculate the derivatives for features.\n",
    "    Inputs:\n",
    "    1) feature_matrix: Data matrix of features (j = 0,...,p)\n",
    "    2) error_vector: A vector of errors of N observations    \n",
    "    \n",
    "    Outputs:\n",
    "    1) derive: Derivative for this feature j    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    derive = -2 * np.dot(feature_matrix.T, error_vector)\n",
    "    \n",
    "    return derive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "features = np.array([[2.0, 1.0], [4.0, 3.0]])\n",
    "weights = np.ones((2, 1))\n",
    "true_outputs = np.array([[1.5], [2.5]])\n",
    "predict_outputs = predict(features, weights)\n",
    "errors = true_outputs - predict_outputs\n",
    "\n",
    "print \"True output is: \" \n",
    "print true_outputs\n",
    "print \"Predicted output is: \"\n",
    "print predict_outputs\n",
    "print \"Error is: \"\n",
    "print errors\n",
    "print \"Derivative is: \"\n",
    "print derivative(features, errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Procedures for Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: Initialize $ \\beta = (\\beta_{0},\\dots,\\beta_{p}) $; <br/>\n",
    "Step 1: Calculate $ \\beta_{j} \\leftarrow \\beta_{j} - Stepsize\\times(-2\\varepsilon^{T}x_j) $, for j = 1,...,p; <br/>\n",
    "Step 2: If not converged, go back to Step 1; <br/>\n",
    "Step 3: Get $ \\beta $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_GD(initial_weights, feature_matrix, true_output, step_size, tolerance, n_iter):\n",
    "    '''This function is used to iteratively calculate OLS estimators for multiple regression model using Gradient Descent.\n",
    "    Inputs:\n",
    "    1) initial_weights: Initial regression coefficients\n",
    "    2) feature_matrix: A matrix of selected features\n",
    "    3) true_output: A vector of true outputs\n",
    "    4) step_size: Size of step for each iteration of gradient search\n",
    "    5) tolerance: Indicate converging condition\n",
    "    6) n_iter: Maximum number of iterations\n",
    "    \n",
    "    Outputs:\n",
    "    1) weights: OLS coefficients.    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    weights = np.array(initial_weights, dtype=np.float64)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Calculate predictions\n",
    "        prediction = predict(feature_matrix, weights)\n",
    "        \n",
    "        # Calculate errors\n",
    "        error = true_output - prediction\n",
    "        \n",
    "        # Calculating derivatives for weights\n",
    "        derivative_vector = derivative(feature_matrix, error)\n",
    "        \n",
    "        # Updating weights\n",
    "        weights -= step_size * derivative_vector        \n",
    "        \n",
    "        # Converging conditions: L2 norm for derivatives\n",
    "        sum_squared_gradient = np.sum(np.power(derivative_vector,2))\n",
    "        \n",
    "        # If converging?\n",
    "        if np.sqrt(sum_squared_gradient) < tolerance:\n",
    "            return weights\n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Convert Pandas DataFrame to Numpy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to: <br/>\n",
    "1. Convert Pandas DataFrame into a Numpy Array/Matrix to do internal calculations;\n",
    "2. Augment this Array/Matrix by adding 1's column in the first column, in order to calculate the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(df, feature_names, output_name):\n",
    "    '''This function is used to convert pandas dataframe to numpy array/matrix, and augment it with 1's column as intercept.\n",
    "    Inputs:\n",
    "    1) df: Original data in the format of pandas dataframe\n",
    "    2) feature_names: A list of names of selected features\n",
    "    3) output_name: Name of selected outputs\n",
    "    \n",
    "    Outputs:\n",
    "    1) feature_matrix: Augmented feature matrix\n",
    "    2) output_vector: A vector of true outputs    \n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    # Feature names of Augmented matrix \n",
    "    augment_feature_names = ['intercept'] + feature_names\n",
    "    \n",
    "    # Augmented feature matrix by adding constant 1's as intercept term, and reorder the feature matrix\n",
    "    df['intercept'] = 1 \n",
    "    feature_matrix = df[augment_feature_names]\n",
    "    n, k = feature_matrix.shape # n: number of observations; k: number of weights\n",
    "    \n",
    "    # Convert selected feature matrix and output vector to Numpy Array\n",
    "    feature_matrix = feature_matrix.values.reshape((n, k))\n",
    "    output_vector = df[output_name].values.reshape((n, 1))\n",
    "\n",
    "    return (feature_matrix, output_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Load Dataset and Conduct OLS Estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset can be obtained from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Iris).\n",
    "\n",
    "### 4.1 Attribute Information:\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "5. class: <br/>\n",
    "-- Iris Setosa <br/>\n",
    "-- Iris Versicolour <br/>\n",
    "-- Iris Virginica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset into Python Pandas DataFrame\n",
    "filepath = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "colnames = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "data = pd.read_csv(filepath, header=None, names=colnames, dtype={'sepal_length':np.float64, 'sepal_width':np.float64, 'petal_length':np.float64, 'petal_width':np.float64})\n",
    "data.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna() # Drop null values\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and output\n",
    "feature_list =  ['sepal_width', 'petal_length']\n",
    "output_label = 'sepal_length'\n",
    "\n",
    "features = convert_data(data, feature_list, output_label)[0]\n",
    "output = convert_data(data, feature_list, output_label)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up initial parameters and try gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up initial parameters\n",
    "initial_weights = np.zeros((len(feature_list)+1, 1))\n",
    "step_size = 7e-5 # Large step_size may lead to overflow\n",
    "tolerance = 2e-9\n",
    "iteration = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try OLS with gradient descent\n",
    "coefficients = OLS_GD(initial_weights, features, output, step_size, tolerance, iteration)\n",
    "print 'Intercept ','X1', 'X2'\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try different `initial_weights`, `step_size`, `tolerance`, or `iteration`.<br/>\n",
    "You can also use different features to run regression model. <br/>\n",
    "Note that large step_size may lead to overflow issue. <br/>\n",
    "Small tolerance may increase the number of iterations, but will result in more accurate estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Compare with OLS Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember in the lecture class, we show the regression expression for OLS estimations:\n",
    "$$ \\beta = (X^{T}X)^{-1}X^{T}Y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_Expression(feature_matrix, true_output):\n",
    "    '''This function is used to calculate OLS coefficients using standard expression.\n",
    "    Inputs:\n",
    "    1) feature_matrix: A matrix of selected features\n",
    "    2) true_output: A vector of true outputs\n",
    "        \n",
    "    Outputs:\n",
    "    1) weights: OLS coefficients\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    XTX = np.dot(feature_matrix.T, feature_matrix)\n",
    "    XTY = np.dot(feature_matrix.T, true_output)\n",
    "    \n",
    "    weights = np.dot(np.linalg.inv(XTX), XTY)\n",
    "    \n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = OLS_Expression(features, output)\n",
    "print 'Intercept ','X1', 'X2'\n",
    "print coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare OLS using gradient descent with OLS using expression. They are very close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Try Open-Source Package\n",
    "### 5.1 Scikit-Learn\n",
    "The package `scikit-learn` can be found at http://scikit-learn.org/stable/index.html. <br/>\n",
    "Please install the package first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "reg.fit(X=data[feature_list], y=data[output_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print reg.intercept_, reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with OLS using gradient descent and OLS using expression. They are very close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can calculate $ R^2 $, which measures goodness-of-fit of regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'R_squared is: ', reg.score(X=data[feature_list], y=data[output_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(data[feature_list[0]], data[output_label], color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[feature_list[1]], reg.predict(data[feature_list]), color='blue', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about Multiple Linear Regression can be found at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use Stochastic Gradient Descent to do gradient search?\n",
    "Hint: The procedure for stochastic gradient descent: <br/>\n",
    "* Initialize all weights\n",
    "* Repeat until convergence or stopping condition is satisfied: <br/>\n",
    "    Randomly shuffle the dataset <br/>\n",
    "    For each piece of data in the dataset: <br/>\n",
    "    (1) Calculate the derivative of one piece of data <br/>\n",
    "    (2) Update the weights by $stepsize \\times derivative$ <br/>\n",
    "    \n",
    "Online resources: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "\n",
    "#### How to use Mini-Batch Gradient Descent to do gradient search?\n",
    "\n",
    "Hint: The general procedure is similar to stochastic descent, except that mini-batch uses multiple pieces of data to calculate derivative and update the weights, rather than one piece of data.\n",
    "\n",
    "Online resources: http://cs229.stanford.edu/notes/cs229-notes1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 References\n",
    "\n",
    "[1] Jason Brownlee, 2018, [Machine Learning Algorithms from Scratch with Python](https://machinelearningmastery.com/machine-learning-algorithms-from-scratch/). <br/>\n",
    "[2] Peter Harrington, 2012. Machine Learning in Action. Shelter Island, NY: Manning Publications Co."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
