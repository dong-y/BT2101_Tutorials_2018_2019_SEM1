{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Introduction to Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal\n",
    "\n",
    "In this notebook, we will explore **Decision Tree** including:\n",
    "* User-defined functions\n",
    "* Open-source package: `scikit-learn`\n",
    "\n",
    "For the **Decision Tree** method, you will:\n",
    "* Use numpy to write functions\n",
    "* Write binary recursive splitting functions\n",
    "* Write decision functions\n",
    "* Write pruning functions\n",
    "* Use open-source package to do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, log\n",
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Summary of Classification Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree\n",
    "A typical classification tree looks like this:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/750/1*2jnsFCe0YmRjb8EvVAo93w.gif\" width=\"500\">\n",
    "\n",
    "#### Steps for Binary Splitting (E.g., Entropy)\n",
    "1. Compute the entropy for data-set;\n",
    "2. For every attribute/feature, calculate information gain for this attribute;\n",
    "3. Pick the feature with highest information gain;\n",
    "4. Repeat until we get the tree we desired;\n",
    "\n",
    "#### Entropy and Information Gain\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*EoWJ8bxc-iqBS-dF-XxsBA.jpeg\" width=\"900\">\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*wQjVzx7zCVb87htqk46vUA.jpeg\" width=\"900\">\n",
    "\n",
    "#### Alternative Criterion for Binary Splitting\n",
    "There are a few possible criteria we can use for selecting features and making the binary splits of classification decision tree:\n",
    "* Classification Error Rate\n",
    "* Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Case: Kaggle Competition - Lending Club Loan Status\n",
    "### 3.1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "The file \"LoanStats_2018Q1.csv\" contains complete loan data for all loans issued through the 2018 Quarter-1, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others.  <br/>\n",
    "Please see https://www.kaggle.com/wendykan/lending-club-loan-data/home.\n",
    "\n",
    "#### Attributes\n",
    "The dataset can be downloaded [here](https://www.lendingclub.com/info/download-data.action). Information on the columns and features can be found in data dictionary. A data dictionary is provided in a separate file \"LCDataDictionary.xlsx\".\n",
    "\n",
    "#### Goal\n",
    "Our goal is to show how to do binary splitting and tree pruning for a classification tree.\n",
    "\n",
    "#### Selected Features\n",
    "For the sake of simplicity, We only select 3 categorical variables as features. We will further transform these categorical variables into binary ones. You need to learn how to fit decision trees when features are continuous variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 1. Calculating entropy value of a given tree node with labels of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(sample_labels):\n",
    "    '''This function is used to calculate entropy value of a given tree node, in which there are samples with labels (0, 1) or (-1, 1).\n",
    "    Inputs:\n",
    "    1) sample_labels: Labels for samples in the current tree node, such as (1, 0, 0, 1, 0) or (1, -1, -1, 1, 0)\n",
    "    \n",
    "    Outputs:\n",
    "    1) entropy: Entropy value of labels in the current tree node.       \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Assert np.array\n",
    "    sample_labels = np.array(sample_labels)\n",
    "    \n",
    "    # What if sample_labels are empty\n",
    "    if sample_labels.size == 0:\n",
    "        return 0  \n",
    "    \n",
    "    # What if all the labels are the same\n",
    "    class_values = np.unique(sample_labels) # Sample labels/classes; Usually (0,1), sometimes (-1,1)\n",
    "    num0 = len(filter(lambda x:x==class_values[0], sample_labels)) # Number of samples with one label\n",
    "    num1 = len(filter(lambda x:x==class_values[1], sample_labels)) if class_values.size > 1 else 0 # Number of samples with another label\n",
    "    \n",
    "    if sample_labels.size == num0 or sample_labels.size == num1:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate entropy value      \n",
    "    p0 = num0 / (num0+num1) # Probability of class 0 labels\n",
    "    p1 = 1 - p0 # Probability of class 1 labels\n",
    "    \n",
    "    entropy = -(p0*log(p0,2) + p1*log(p1,2))    \n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 2. Calculating information gain when a given tree node is splitted by a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(samples, output, feature):\n",
    "    '''This function is used to calculate information gain when a given tree node is splitted by a given feature.\n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    1) output: Name of the output column\n",
    "    2) feature: Name of the feature used to split the current tree node. Remember the features we selected in this case are binary.\n",
    "    \n",
    "    Outputs:\n",
    "    1) information_gain: How much reduction in entropy value if the current tree node is splitted by the feature \n",
    "    2) subsamples[0]: Data samples where feature values are one label (e.g., 0 or -1)\n",
    "    3) subsamples[1]: Data samples where feature values are another label (e.g., 1)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Split samples by feature values into subsamples\n",
    "    subsamples = defaultdict()\n",
    "    entropy_after = 0 # Entropy value after splitting\n",
    "    \n",
    "    for feature_value in np.unique(samples[output]):\n",
    "        subsamples[feature_value] = samples[samples[feature] == feature_value]\n",
    "        temp = subsamples[feature_value] # Store a temporary copy\n",
    "        p = len(temp) / len(samples) # Proportion of this subsample\n",
    "        entropy_after += p * entropy(temp[output])\n",
    "        \n",
    "    # Calculate information gain \n",
    "    information_gain = entropy(samples[output]) - entropy_after\n",
    "    \n",
    "    return (information_gain, subsamples[0], subsamples[1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "a = np.array([[1,0,0,1],[0,1,1,0],[1,1,1,1],[0,0,0,0],[1,1,0,0]])\n",
    "data = pd.DataFrame(a, columns=['x1','x2','x3','y'])\n",
    "info_gain(data, 'y', 'x1')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 3. Decide the best feature to split on: Using information gain and entropy as criterion\n",
    "1. Loop over each feature in the feature list;\n",
    "2. For each loop (feature f), split the data into 2 groups: In group 1 (left split), all samples' feature f has value 0. In group 2 (right split), all samples' feature f has value 1;\n",
    "3. Calculate the information gain for this split;\n",
    "4. If the information gain for this split using this feature is highest, then pick this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature_split(samples, output, features):\n",
    "    '''This function is used to determine the best feature to split based on maximized information gain.\n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) features: A list of feature names\n",
    "    \n",
    "    Outputs:\n",
    "    1) best_feature: The best feature which is used to do binary splitting\n",
    "    2) best_left_split: Data samples where the best feature's values are 0\n",
    "    3) best_right_split: Data samples where the best feature's values are 1      \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Initialize best feature, best information gain value, best left/right split samples\n",
    "    best_feature = None \n",
    "    best_information_gain = 0\n",
    "    best_left_split = None\n",
    "    best_right_split = None    \n",
    "    \n",
    "    samples_row = float(len(samples)) # Number of rows in the data samples\n",
    "    \n",
    "    # Loop through features and find the best feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # Splitting the data samples\n",
    "        current_split = info_gain(samples, output, feature)\n",
    "        information_gain = current_split[0]\n",
    "        left_split = current_split[1]\n",
    "        right_split = current_split[2]\n",
    "        \n",
    "        # Check if this feature is better\n",
    "        if information_gain >= best_information_gain:\n",
    "            best_feature, best_information_gain, best_left_split, best_right_split = (feature, information_gain, left_split, right_split)\n",
    "    \n",
    "    return (best_feature, best_information_gain, best_left_split, best_right_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "a = np.array([[1,0,0,1],[0,1,1,0],[1,1,1,1],[0,0,0,0],[1,1,0,0]])\n",
    "data = pd.DataFrame(a, columns=['x1','x2','x3','y'])\n",
    "best_feature_split(data, 'y', ['x1','x2','x3'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 4. Build our classification tree and do pre-pruning\n",
    "We need to decide stopping conditions (i.e., pre-pruning):\n",
    "1. The samples' labels in the current node are the same (either 0 or 1);\n",
    "2. All the features have already been used for split;\n",
    "3. The current tree has already reached maximum depth **max_depth**;\n",
    "4. The number of samples in the current node is lower than minimum number **min_number**;\n",
    "5. The information gain for the current split is lower than a threshold **min_infogain** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopping Condition 1: The samples' labels in the current node are the same (either 0/-1 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_1(node_labels):\n",
    "    '''This function is used to verify whether stopping condition 1 is satisfied.\n",
    "    Inputs:\n",
    "    1) node_labels: The samples' labels in the current node\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if they are all the same, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # numpy array\n",
    "    node_labels = np.array(node_labels)\n",
    "    \n",
    "    # Empty labels\n",
    "    if len(node_labels) == 0:\n",
    "        return True\n",
    "    \n",
    "    if len(np.unique(node_labels)) == 1:\n",
    "        print \"Stopping Condition 1: The samples' labels in the current node are the same (either 0/-1 or 1)\"\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopping Condition 2: All the features have already been used for split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_2(features):\n",
    "    '''This function is used to verify whether stopping condition 2 is satisfied.\n",
    "    Inputs:\n",
    "    1) features: A list of feature names\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if the feature list is empty, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if len(features) == 0 or features == None:\n",
    "        print \"Stopping Condition 2: All the features have already been used for split\"\n",
    "        return True\n",
    "    else:\n",
    "        return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopping Condition 3: The current tree has already reached maximum depth **max_depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_3(tree_depth, max_depth):\n",
    "    '''This function is used to verify whether stopping condition 3 is satisfied.\n",
    "    Inputs:\n",
    "    1) tree_depth: The depth of the current tree\n",
    "    2) max_depth: Maximum tree depth\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if the current depth reaches maximum depth, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if tree_depth >= max_depth:\n",
    "        print \"Stopping Condition 3: The current tree has already reached maximum depth\"\n",
    "        return True\n",
    "    else:\n",
    "        return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopping Condition 4: The number of samples in the current node is lower than minimum number **min_number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_4(samples, min_number):\n",
    "    '''This function is used to verify whether stopping condition 4 is satisfied.\n",
    "    Inputs:\n",
    "    1) samples: Data samples in the current node (Pandas DataFrame)\n",
    "    2) min_number: Minimum number of node size\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if sample size is smaller than the minimum number, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if samples.size <= min_number:\n",
    "        print \"Stopping Condition 4: The number of samples in the current node is lower than minimum number\"\n",
    "        return True\n",
    "    else:\n",
    "        return False      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopping Condition 5: The information gain for the current split is lower than a threshold **min_infogain** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain(samples, output, feature) -> information gain, left, right\n",
    "# best_feature_split(samples, output, features) -> feature, information gain, left, right\n",
    "def stop_5(info_gain, min_infogain):\n",
    "    '''This function is used to verify whether stopping condition 5 is satisfied.\n",
    "    Inputs:\n",
    "    1) info_gain: Information gain after this best split\n",
    "    2) min_infogain: Minimum information gain\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if information gain after this best splitting is smaller than the minimum number, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if info_gain <= min_infogain:\n",
    "        print \"Stopping Condition 5: The information gain for the current split is lower than a threshold\"\n",
    "        return True\n",
    "    else:\n",
    "        return False      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build classification tree\n",
    "The data structure for the nested tree structure (including temporary tree nodes, and leaf nodes) is shown as:\n",
    "\n",
    "{ <br/>\n",
    "   'label': None for temporary node, or predicted label at the leaf node (e.g., \"Majority Voting\" criterion) for leaf node; <br/>\n",
    "   'left_tree': Left tree after the selected feature (=0 or -1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'right_tree': Right tree after the selected feature (=1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'best_feature': The feature that is selected to do binary split for temporary node, None for leaf node. <br/>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(output_labels):\n",
    "    '''This function is used to get predicted label based on \"Majority Voting\" criterion for the current leaf node.     \n",
    "    Inputs:\n",
    "    1) output_labels: Outputs (labels) in this leaf node, such as [1, 0, 0, 1, 1]\n",
    "    \n",
    "    Outputs:\n",
    "    1) prediction: Predicted label for this leaf node (e.g., 0/-1, or 1)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # numpy array\n",
    "    output_labels = np.array(output_labels)\n",
    "    \n",
    "    # Empty label\n",
    "    if output_labels.size == 0:\n",
    "        return None\n",
    "    \n",
    "    # Count output labels (0/-1 or 1)\n",
    "    values = np.unique(output_labels)\n",
    "    \n",
    "    if len(values) == 1:\n",
    "        return values[0]\n",
    "    else:\n",
    "        num0 = len(output_labels[output_labels == values[0]])\n",
    "        num1 = len(output_labels[output_labels == values[1]])\n",
    "        return values[1] if num1 >= num0 else values[0] # Prediction based on \"Majority Voting\" criterion   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassificationTree(samples, output, features, step, tree_depth, max_depth, min_number, min_infogain):\n",
    "    '''This function is used to build a classification tree in a recursive way.\n",
    "       Remember how you build a binary tree in the previous C++ and Data Structure courses).\n",
    "       \n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) features: A list of feature names\n",
    "    4) step: The current binary split step\n",
    "    5) tree_depth: The depth of the current tree\n",
    "    6) max_depth: Maximum depth this tree can grow\n",
    "    7) min_number: Minimum number of node size\n",
    "    8) min_infogain: Minimum information gain\n",
    "    \n",
    "    Outputs:\n",
    "    1) tree_nodes: Nested tree nodes, which are stored and shown in nested dictionary type    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    current_features = features # Current feature list\n",
    "    labels = samples[output] # Output labels in the current tree node\n",
    "\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"Step %s: Current tree depth is %s. Current tree node has %s data points\" % (step, tree_depth, samples.size)\n",
    "    \n",
    "    # Verify whether stopping conditions 1-4 are satisfied. If satisfied, return a leaf_node\n",
    "    if stop_1(labels) or stop_2(current_features) or stop_3(tree_depth, max_depth) or stop_4(samples, min_number):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None          \n",
    "            \n",
    "                }\n",
    "    \n",
    "    # If pass stopping conditions 1-4, then do best splitting\n",
    "    best_split = best_feature_split(samples, output, current_features)\n",
    "    best_feature, best_infogain, best_left, best_right = (best_split[0], best_split[1], best_split[2], best_split[3])\n",
    "    \n",
    "    # Verify whether stopping condition 5 is satisfied. If satisfied, return a leaf node\n",
    "    if stop_5(best_infogain, min_infogain):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None          \n",
    "            \n",
    "                } \n",
    "    \n",
    "    # If pass stopping condition 5, then move on\n",
    "    step += 1\n",
    "    print \"Step %s: Binary split on %s. Size of Left and Right tree is (%s, %s)\" % (step, best_feature, len(best_left), len(best_right))\n",
    "    current_features.remove(best_feature) # Remove this feature if this feature is used for split\n",
    "    \n",
    "    # Do binary split on left tree and right tree in a recursive way\n",
    "    left_split = ClassificationTree(best_left, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain)\n",
    "    right_split = ClassificationTree(best_right, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain)\n",
    "    \n",
    "    return {\n",
    "            'label': None,\n",
    "            'left_tree': left_split,\n",
    "            'right_tree': right_split,\n",
    "            'best_feature': best_feature        \n",
    "            \n",
    "            }  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Cleaning\n",
    "We need to do some simple data cleaning work for original lend club loan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "loan_data = pd.read_csv(\"./LoanStats_2018Q1.csv\", low_memory=False, header=1)\n",
    "loan_data.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data[\"loan_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and output\n",
    "features = ['grade', 'term', 'home_ownership']       \n",
    "output = 'risky'\n",
    "loan_data = loan_data[loan_data['loan_status'] != 'Current']\n",
    "loan_data[output] = loan_data['loan_status'].map(lambda x: 1 if x in ['Late (31-120 days)', 'Late (16-30 days)', 'Charged Off'] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loan_data[features+[output]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform categorical features to binary features\n",
    "grade_dummy = pd.get_dummies(dataset['grade'], prefix='grade')  \n",
    "term_dummy = pd.get_dummies(dataset['term'], prefix='term')\n",
    "home_ownership_dummy = pd.get_dummies(dataset['home_ownership'], prefix='home_ownership')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.join([grade_dummy, term_dummy, home_ownership_dummy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna() # Remove all missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update our features and output\n",
    "features = list(dataset.columns[2:])\n",
    "output = dataset.columns[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Classification and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose max_depth = 6; min_infogain=5e-4\n",
    "features = list(dataset.columns[2:])\n",
    "output = dataset.columns[1]\n",
    "tree_model = ClassificationTree(dataset, output, features, step=0, tree_depth=0, max_depth=6, min_number=5, min_infogain=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try different initial parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to predict new samples' labels. <br/>\n",
    "\n",
    "Remember our tree structure is like: <br/>\n",
    "{ <br/>\n",
    "   'label': None for temporary node, or predicted label at the leaf node (e.g., \"Majority Voting\" criterion) for leaf node; <br/>\n",
    "   'left_tree': Left tree after the selected feature (=0 or -1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'right_tree': Right tree after the selected feature (=1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'best_feature': The feature that is selected to do binary split for temporary node, None for leaf node. <br/>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(new_sample, train_tree):   \n",
    "    '''This function is used to predict the label of one new sample.\n",
    "    Inputs:\n",
    "    1) new_sample: A new sample, we would like to predict its label (Pandas DataFrame)\n",
    "    2) train_tree: The classification tree we have just trained\n",
    "    \n",
    "    Outputs:\n",
    "    1) predict_label: The predicted label for this new sample  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # If move to the leaf node\n",
    "    if train_tree['best_feature']==None:\n",
    "        return train_tree['label']\n",
    "    \n",
    "    # If still stay at temporary node\n",
    "    else:\n",
    "        # Find the value of the best feature in the current node\n",
    "        # If value is 0, then go to left tree\n",
    "        # If value is 1, then go to right tree\n",
    "        # Remember what your have learned in Data Structure course, about binary tree\n",
    "        best_feature = train_tree['best_feature']\n",
    "        return predict_label(new_sample, train_tree['left_tree']) if new_sample[best_feature]==0 else predict_label(new_sample, train_tree['right_tree'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to learn partial and apply function. They are powerful.\n",
    "from functools import partial\n",
    "prediction = partial(predict_label, train_tree=tree_model)\n",
    "predicted_labels = dataset.apply(lambda x: prediction(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate predicted_labels into our dataset\n",
    "dataset['prediction'] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it is your turn: \n",
    "1. Set your own stopping conditions for pre-pruning;\n",
    "2. Try to do post-pruning;\n",
    "3. Try to use different criteria for binary split, such as misclassification and Gini index;\n",
    "4. Write a function to calculate misclassification error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Open-Source Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a break and let us use open-source package to run decision tree models. <br/>\n",
    "Use `Scikit-learn`to make classification trees and make predictions: http://scikit-learn.org/stable/modules/tree.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update our features and output\n",
    "features = list(dataset.columns[2:])\n",
    "output = dataset.columns[1]\n",
    "\n",
    "# Split dataset to do validation\n",
    "X = dataset[features]\n",
    "y = dataset[output]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on train data\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree_model = decision_tree.fit(X_train, y_train)\n",
    "decision_tree_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted labels for test data\n",
    "y_pred = decision_tree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print cm\n",
    "print TN, FP, FN, TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of decision tree model\n",
    "print \"Accuracy: \", accuracy_score(y_test, y_pred)\n",
    "print \"Sensitivity: \", recall_score(y_test, y_pred)\n",
    "print \"Precision: \", precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate:\n",
    "1. Accuracy\n",
    "2. Misclassification rate\n",
    "3. Precision\n",
    "4. Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get predicted scores Pr(y=1): Used as thresholds for calculating TP Rate and FP Rate\n",
    "score = decision_tree_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, score) # fpr: FP Rate, tpr: TP Rate, thresholds: Pr(y=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree\n",
    "# Remember you should install package graphviz first\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(decision_tree_model, out_file=None, feature_names=features, class_names=output, \n",
    "                                filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in .pdf \n",
    "graph.render(\"Lending Club Loan Status\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 What if features are continuous?\n",
    "### 5.2 What if output is continuous? \n",
    "* Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 References\n",
    "\n",
    "[1] Jason Brownlee, 2018, [Machine Learning Algorithms from Scratch with Python](https://machinelearningmastery.com/machine-learning-algorithms-from-scratch/). <br/>\n",
    "[2] Peter Harrington, 2012. Machine Learning in Action. Shelter Island, NY: Manning Publications Co."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
