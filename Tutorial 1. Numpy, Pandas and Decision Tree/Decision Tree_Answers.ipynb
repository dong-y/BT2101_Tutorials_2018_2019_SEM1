{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Introduction to Decision Tree\n",
    "\n",
    "# This assignment should be run in Python-2.7 version. If you run in Python 3+, you will get errors. In Anaconda, you could easily create a new virtual environment with Python-2.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal\n",
    "\n",
    "In this notebook, we will explore **Decision Tree** including:\n",
    "* User-defined functions\n",
    "* Open-source package: `scikit-learn`\n",
    "\n",
    "For the **Decision Tree** method, you will:\n",
    "* Use numpy to write functions\n",
    "* Write binary recursive splitting functions\n",
    "* Write decision functions\n",
    "* Write pruning functions\n",
    "* Use open-source package to do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, log\n",
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Summary of Classification Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Tree\n",
    "A typical classification tree looks like this:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/750/1*2jnsFCe0YmRjb8EvVAo93w.gif\" width=\"500\">\n",
    "\n",
    "#### Steps for Binary Splitting (E.g., Entropy)\n",
    "1. Compute the entropy for data-set;\n",
    "2. For every attribute/feature, calculate information gain for this attribute;\n",
    "3. Pick the feature with highest information gain;\n",
    "4. Repeat until we get the tree we desired;\n",
    "\n",
    "#### Entropy and Information Gain\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*EoWJ8bxc-iqBS-dF-XxsBA.jpeg\" width=\"900\">\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*wQjVzx7zCVb87htqk46vUA.jpeg\" width=\"900\">\n",
    "\n",
    "#### Alternative Criterion for Binary Splitting\n",
    "There are a few possible criteria we can use for selecting features and making the binary splits of classification decision tree:\n",
    "* Classification Error Rate\n",
    "* Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Case: Kaggle Competition - Lending Club Loan Status\n",
    "### 3.1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "The file \"LoanStats_2018Q1.csv\" contains complete loan data for all loans issued through the 2018 Quarter-1, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others.  <br/>\n",
    "Please see https://www.kaggle.com/wendykan/lending-club-loan-data/home.\n",
    "\n",
    "#### Attributes\n",
    "The dataset can be downloaded [here](https://www.lendingclub.com/info/download-data.action). Information on the columns and features can be found in data dictionary. A data dictionary is provided in a separate file \"LCDataDictionary.xlsx\".\n",
    "\n",
    "#### Goal\n",
    "Our goal is to show how to do binary splitting and tree pruning for a classification tree.\n",
    "\n",
    "#### Selected Features\n",
    "For the sake of simplicity, We only select 3 categorical variables as features. We will further transform these categorical variables into binary ones. You need to learn how to fit decision trees when features are continuous variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 1. Calculating entropy value of a given tree node with labels of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(sample_labels):\n",
    "    '''This function is used to calculate entropy value of a given tree node, in which there are samples with labels (0, 1) or (-1, 1).\n",
    "    Inputs:\n",
    "    1) sample_labels: Labels for samples in the current tree node, such as (1, 0, 0, 1, 0) or (1, -1, -1, 1, 0)\n",
    "    \n",
    "    Outputs:\n",
    "    1) entropy: Entropy value of labels in the current tree node.       \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Assert np.array\n",
    "    sample_labels = np.array(sample_labels)\n",
    "    \n",
    "    # What if sample_labels are empty\n",
    "    if sample_labels.size == 0:\n",
    "        return 0  \n",
    "    \n",
    "    # What if all the labels are the same\n",
    "    class_values = np.unique(sample_labels) # Sample labels/classes; Usually (0,1), sometimes (-1,1)\n",
    "    num0 = len(filter(lambda x:x==class_values[0], sample_labels)) # Number of samples with one label\n",
    "    num1 = len(filter(lambda x:x==class_values[1], sample_labels)) if class_values.size > 1 else 0 # Number of samples with another label\n",
    "    \n",
    "    if sample_labels.size == num0 or sample_labels.size == num1:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate entropy value      \n",
    "    p0 = num0 / (num0+num1) # Probability of class 0 labels\n",
    "    p1 = 1 - p0 # Probability of class 1 labels\n",
    "    \n",
    "    entropy = -(p0*log(p0,2) + p1*log(p1,2))    \n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 2. Calculating information gain when a given tree node is splitted by a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(samples, output, feature):\n",
    "    '''This function is used to calculate information gain when a given tree node is splitted by a given feature.\n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    1) output: Name of the output column\n",
    "    2) feature: Name of the feature used to split the current tree node. Remember the features we selected in this case are binary.\n",
    "    \n",
    "    Outputs:\n",
    "    1) information_gain: How much reduction in entropy value if the current tree node is splitted by the feature \n",
    "    2) subsamples[0]: Data samples where feature values are one label (e.g., 0 or -1)\n",
    "    3) subsamples[1]: Data samples where feature values are another label (e.g., 1)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Split samples by feature values into subsamples\n",
    "    subsamples = defaultdict()\n",
    "    entropy_after = 0 # Entropy value after splitting\n",
    "    \n",
    "    for feature_value in np.unique(samples[feature]):\n",
    "        subsamples[feature_value] = samples[samples[feature] == feature_value]\n",
    "        temp = subsamples[feature_value] # Store a temporary copy\n",
    "        p = len(temp) / len(samples) # Proportion of this subsample\n",
    "        entropy_after += p * entropy(temp[output])\n",
    "        \n",
    "    # Calculate information gain \n",
    "    information_gain = entropy(samples[output]) - entropy_after\n",
    "    \n",
    "    # Left or Right subtree may be None  \n",
    "    return (information_gain, subsamples[0] if 0 in subsamples else None, subsamples[1] if 1 in subsamples else None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "a = np.array([[1,0,0,1],[0,1,1,0],[1,1,1,1],[0,0,0,0],[1,1,0,0]])\n",
    "data = pd.DataFrame(a, columns=['x1','x2','x3','y'])\n",
    "info_gain(data, 'y', 'x1')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "a = np.array([[1,0,0,1],[1,1,1,0],[1,1,1,1],[1,0,0,0],[1,1,0,0]])\n",
    "data = pd.DataFrame(a, columns=['x1','x2','x3','y'])\n",
    "info_gain(data, 'y', 'x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is information gain nonnegative?\n",
    "Math Proof: https://www.cs.cmu.edu/~ggordon/780-fall07/fall06/homework/15780f06-hw4sol.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 3. Decide the best feature to split on: Using information gain and entropy as criterion\n",
    "1. Loop over each feature in the feature list;\n",
    "2. For each loop (feature f), split the data into 2 groups: In group 1 (left split), all samples' feature f has value 0. In group 2 (right split), all samples' feature f has value 1;\n",
    "3. Calculate the information gain for this split;\n",
    "4. If the information gain for this split using this feature is highest, then pick this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature_split(samples, output, features):\n",
    "    '''This function is used to determine the best feature to split based on maximized information gain.\n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) features: A list of feature names\n",
    "    \n",
    "    Outputs:\n",
    "    1) best_feature: The best feature which is used to do binary splitting\n",
    "    2) best_left_split: Data samples where the best feature's values are 0\n",
    "    3) best_right_split: Data samples where the best feature's values are 1      \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Initialize best feature, best information gain value, best left/right split samples\n",
    "    best_feature = None \n",
    "    best_information_gain = 0\n",
    "    best_left_split = None\n",
    "    best_right_split = None    \n",
    "    \n",
    "    samples_row = float(len(samples)) # Number of rows in the data samples\n",
    "    \n",
    "    # Loop through features and find the best feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # Splitting the data samples\n",
    "        current_split = info_gain(samples, output, feature)\n",
    "        information_gain = current_split[0]\n",
    "        left_split = current_split[1]\n",
    "        right_split = current_split[2]\n",
    "        \n",
    "        # Check if this feature is better\n",
    "        if information_gain >= best_information_gain:\n",
    "            best_feature, best_information_gain, best_left_split, best_right_split = feature, information_gain, left_split, right_split\n",
    "    \n",
    "    return (best_feature, best_information_gain, best_left_split, best_right_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "a = np.array([[1,0,0,1],[0,1,1,0],[1,1,1,1],[0,0,0,0],[1,1,0,0]])\n",
    "data = pd.DataFrame(a, columns=['x1','x2','x3','y'])\n",
    "best_feature_split(data, 'y', ['x1','x2','x3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,0,0,1],[1,1,1,0],[1,1,1,1],[1,0,0,0],[1,1,0,0]])\n",
    "data = pd.DataFrame(a, columns=['x1','x2','x3','y'])\n",
    "best_feature_split(data, 'y', ['x1','x2','x3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 4. Build our classification tree and do pre-pruning\n",
    "We need to decide stopping conditions (i.e., pre-pruning):\n",
    "1. The samples' labels in the current node are the same (either 0 or 1);\n",
    "2. All the features have already been used for split;\n",
    "3. The current tree has already reached maximum depth **max_depth**;\n",
    "4. The number of samples in the current node is lower than minimum number **min_number**;\n",
    "5. The information gain for the current split is lower than a threshold **min_infogain** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopping Condition 1: The samples' labels in the current node are the same (either 0/-1 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_1(node_labels):\n",
    "    '''This function is used to verify whether stopping condition 1 is satisfied.\n",
    "    Inputs:\n",
    "    1) node_labels: The samples' labels in the current node\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if they are all the same, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # numpy array\n",
    "    node_labels = np.array(node_labels)\n",
    "    \n",
    "    # Empty labels\n",
    "    if len(node_labels) == 0:\n",
    "        return True\n",
    "    \n",
    "    if len(np.unique(node_labels)) == 1:\n",
    "        print \"Stopping Condition 1: The samples' labels in the current node are the same (either 0/-1 or 1)\"\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopping Condition 2: All the features have already been used for split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_2(features):\n",
    "    '''This function is used to verify whether stopping condition 2 is satisfied.\n",
    "    Inputs:\n",
    "    1) features: A list of feature names\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if the feature list is empty, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if len(features) == 0 or features == None:\n",
    "        print \"Stopping Condition 2: All the features have already been used for split\"\n",
    "        return True\n",
    "    else:\n",
    "        return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopping Condition 3: The current tree has already reached maximum depth **max_depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_3(tree_depth, max_depth):\n",
    "    '''This function is used to verify whether stopping condition 3 is satisfied.\n",
    "    Inputs:\n",
    "    1) tree_depth: The depth of the current tree\n",
    "    2) max_depth: Maximum tree depth\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if the current depth reaches maximum depth, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if tree_depth >= max_depth:\n",
    "        print \"Stopping Condition 3: The current tree has already reached maximum depth\"\n",
    "        return True\n",
    "    else:\n",
    "        return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopping Condition 4: The number of samples in the current node is lower than minimum number **min_number**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_4(samples, min_number):\n",
    "    '''This function is used to verify whether stopping condition 4 is satisfied.\n",
    "    Inputs:\n",
    "    1) samples: Data samples in the current node (Pandas DataFrame)\n",
    "    2) min_number: Minimum number of node size\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if sample size is smaller than the minimum number, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if samples.size <= min_number:\n",
    "        print \"Stopping Condition 4: The number of samples in the current node is lower than minimum number\"\n",
    "        return True\n",
    "    else:\n",
    "        return False      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopping Condition 5: The information gain for the current split is lower than a threshold **min_infogain** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain(samples, output, feature) -> information gain, left, right\n",
    "# best_feature_split(samples, output, features) -> feature, information gain, left, right\n",
    "def stop_5(info_gain, min_infogain):\n",
    "    '''This function is used to verify whether stopping condition 5 is satisfied.\n",
    "    Inputs:\n",
    "    1) info_gain: Information gain after this best split\n",
    "    2) min_infogain: Minimum information gain\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if information gain after this best splitting is smaller than the minimum number, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if info_gain <= min_infogain:\n",
    "        print \"Stopping Condition 5: The information gain for the current split is lower than a threshold\"\n",
    "        return True\n",
    "    else:\n",
    "        return False      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build classification tree\n",
    "The data structure for the nested tree structure (including temporary tree nodes, and leaf nodes) is shown as:\n",
    "\n",
    "{ <br/>\n",
    "   'label': None for temporary node, or predicted label at the leaf node (e.g., \"Majority Voting\" criterion) for leaf node; <br/>\n",
    "   'left_tree': Left tree after the selected feature (=0 or -1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'right_tree': Right tree after the selected feature (=1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'best_feature': The feature that is selected to do binary split for temporary node, None for leaf node. <br/>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(output_labels):\n",
    "    '''This function is used to get predicted label based on \"Majority Voting\" criterion for the current leaf node.     \n",
    "    Inputs:\n",
    "    1) output_labels: Outputs (labels) in this leaf node, such as [1, 0, 0, 1, 1]\n",
    "    \n",
    "    Outputs:\n",
    "    1) prediction: Predicted label for this leaf node (e.g., 0/-1, or 1)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # numpy array\n",
    "    output_labels = np.array(output_labels)\n",
    "    \n",
    "    # Empty label\n",
    "    if output_labels.size == 0:\n",
    "        return None\n",
    "    \n",
    "    # Count output labels (0/-1 or 1)\n",
    "    values = np.unique(output_labels)\n",
    "    \n",
    "    if len(values) == 1:\n",
    "        return values[0]\n",
    "    else:\n",
    "        num0 = len(output_labels[output_labels == values[0]])\n",
    "        num1 = len(output_labels[output_labels == values[1]])\n",
    "        return values[1] if num1 >= num0 else values[0] # Prediction based on \"Majority Voting\" criterion   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassificationTree(samples, output, features, step, tree_depth, max_depth, min_number, min_infogain):\n",
    "    '''This function is used to build a classification tree in a recursive way.\n",
    "       Remember how you build a binary tree in the previous C++ and Data Structure courses).\n",
    "       \n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) features: A list of feature names\n",
    "    4) step: The current binary split step\n",
    "    5) tree_depth: The depth of the current tree\n",
    "    6) max_depth: Maximum depth this tree can grow\n",
    "    7) min_number: Minimum number of node size\n",
    "    8) min_infogain: Minimum information gain\n",
    "    \n",
    "    Outputs:\n",
    "    1) tree_nodes: Nested tree nodes, which are stored and shown in nested dictionary type    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # If samples are empty, return None\n",
    "    if samples is None or len(samples)==0:\n",
    "        return None\n",
    "           \n",
    "    \n",
    "    current_features = features # Current feature list\n",
    "    labels = samples[output] # Output labels in the current tree node\n",
    "\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"Step %s: Current tree depth is %s. Current tree node has %s data points\" % (step, tree_depth, len(samples)) # Sample size\n",
    "    \n",
    "    # Verify whether stopping conditions 1-4 are satisfied. If satisfied, return a leaf_node\n",
    "    if stop_1(labels) or stop_2(current_features) or stop_3(tree_depth, max_depth) or stop_4(samples, min_number):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None          \n",
    "            \n",
    "                }\n",
    "    \n",
    "    # If pass stopping conditions 1-4, then do best splitting\n",
    "    best_split = best_feature_split(samples, output, current_features)\n",
    "    best_feature, best_infogain, best_left, best_right = best_split[0], best_split[1], best_split[2], best_split[3]\n",
    "    \n",
    "    # Verify whether stopping condition 5 is satisfied. If satisfied, return a leaf node\n",
    "    if stop_5(best_infogain, min_infogain):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None          \n",
    "            \n",
    "                } \n",
    "    \n",
    "    # If pass stopping condition 5, then move on\n",
    "    step += 1\n",
    "    print \"Step %s: Binary split on %s. Size of Left and Right tree is (%s, %s)\" % \\\n",
    "          (step, best_feature, len(best_left) if best_left is not None else 0, len(best_right) if best_right is not None else 0)\n",
    "    current_features.remove(best_feature) # Remove this feature if this feature is used for split\n",
    "    \n",
    "    # Do binary split on left tree and right tree in a recursive way\n",
    "    left_split = ClassificationTree(best_left, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain) \n",
    "    right_split = ClassificationTree(best_right, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain) \n",
    "    \n",
    "    return {\n",
    "            'label': None,\n",
    "            'left_tree': left_split,\n",
    "            'right_tree': right_split,\n",
    "            'best_feature': best_feature        \n",
    "            \n",
    "            }  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Cleaning\n",
    "\n",
    "### You could skip entire 3.3 because data cleaning is not a main issue for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "loan_data = pd.read_csv(\"./LoanStats_2018Q1.csv\", low_memory=False, header=1)\n",
    "loan_data.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and output\n",
    "features = ['grade', 'term', 'home_ownership']       \n",
    "output = 'risky'\n",
    "loan_data = loan_data[loan_data['loan_status'] != 'Current']\n",
    "loan_data[output] = loan_data['loan_status'].map(lambda x: 1 if x in ['Late (31-120 days)', 'Late (16-30 days)', 'Charged Off'] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loan_data[features+[output]]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform categorical features to binary features\n",
    "grade_dummy = pd.get_dummies(dataset['grade'], prefix='grade')  \n",
    "term_dummy = pd.get_dummies(dataset['term'], prefix='term')\n",
    "home_ownership_dummy = pd.get_dummies(dataset['home_ownership'], prefix='home_ownership')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.join([grade_dummy, term_dummy, home_ownership_dummy])\n",
    "dataset = dataset.drop(features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna() # Remove all missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index() # Reset indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print dataset[output].value_counts() # Very unbalanced dataset\n",
    "dataset.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that you need to do oversampling on rare outputs (which is '1' in this example)\n",
    "## Otherwise the tree nodes will always predict 0 (according to majority voting method)\n",
    "## That will be problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update our features and output\n",
    "\n",
    "import copy\n",
    "dataset_copy = copy.deepcopy(dataset) # Please note the difference between shallow copy and deep copy in Python\n",
    "\n",
    "features = list(dataset_copy.columns[2:])\n",
    "output = dataset_copy.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address Unbalanced data: Oversampling\n",
    "\n",
    "# You may need to set seed first, otherwise if you use different training data to train the model, you will get different tree model\n",
    "# and you may get different post-pruning results\n",
    "import random\n",
    "random.seed(12345)\n",
    "\n",
    "# Class count\n",
    "count_class_0, count_class_1 = dataset_copy[output].value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = dataset_copy[dataset_copy[output] == 0]\n",
    "df_class_1 = dataset_copy[dataset_copy[output] == 1]\n",
    "\n",
    "# Address unblanced data issue: Oversampling on data samples with rare outputs (which is '1' in this example)\n",
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True, random_state=12345)\n",
    "dataset_copy = pd.concat([df_class_1_over, df_class_0], axis=0)\n",
    "\n",
    "dataset_copy[output].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy.to_csv('./dataset.csv', index=None) # Store a copy: Export dataset to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Classification and Simple Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose max_depth = 6; min_infogain=5e-4\n",
    "# You can set other parameter values\n",
    "features = list(dataset_copy.columns[2:])\n",
    "output = dataset_copy.columns[1]\n",
    "tree_model = ClassificationTree(dataset_copy, output, features, step=0, tree_depth=0, max_depth=7, min_number=5, min_infogain=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Tree\n",
    "\n",
    "The data structure for the nested tree structure (including temporary tree nodes, and leaf nodes) is shown as:\n",
    "\n",
    "{ <br/>\n",
    "   'label': None for temporary node, or predicted label at the leaf node (e.g., \"Majority Voting\" criterion) for leaf node; <br/>\n",
    "   'left_tree': Left tree after the selected feature (=0 or -1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'right_tree': Right tree after the selected feature (=1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'best_feature': The feature that is selected to do binary split for temporary node, None for leaf node. <br/>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(tree, depth=0, LR=0):\n",
    "    '''This function is used to visualize the tree model\n",
    "    \n",
    "    Inputs:\n",
    "    1) tree: tree model\n",
    "    2) depth: \n",
    "    3) LR: Left_subtree: feature=0; Right_subtree: feature=1\n",
    "    \n",
    "    Outputs:\n",
    "    1) Print the tree model structure (i.e., nested dictionary)      \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if depth==0: # Root node\n",
    "        print tree['best_feature']\n",
    "        print_tree(tree['left_tree'], depth+1, 0)\n",
    "        print_tree(tree['right_tree'], depth+1, 1)\n",
    "        \n",
    "    else:\n",
    "        if tree['best_feature'] is not None: # Not leaf node\n",
    "            print \"\\t\" * depth, \"=%s :\" %(LR), tree['best_feature']\n",
    "            try:\n",
    "                print_tree(tree['left_tree'], depth+1, 0)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                print_tree(tree['right_tree'], depth+1, 1)   \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        else:\n",
    "            print \"\\t\" * depth, \"=%s : -\" %(LR), \"(Predict %s)\" %(tree['label']) # Leaf node      \n",
    "                \n",
    "print_tree(tree_model, depth=0, LR=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignments:\n",
    "\n",
    "Set your own stopping conditions for pre-pruning, For example: \n",
    "* You can adjust initial parameter values for these stopping conditions.\n",
    "* Limiting the number of binary-split.\n",
    "\n",
    "Choose one of them and describe or write your function/code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose: Limiting the number of binary-split\n",
    "# Note that python cannot define a static variable (unless you define it in python \"Class\" method)\n",
    "# You can also count how many variables have already been used for splitting. This number is equal to the number of splits\n",
    "\n",
    "def ClassificationTree(samples, output, features, step, tree_depth, max_depth, min_number, min_infogain, counter, max_split):\n",
    "    '''This function is used to build a classification tree in a recursive way.\n",
    "       Remember how you build a binary tree in the previous C++ and Data Structure courses).\n",
    "       \n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) features: A list of feature names\n",
    "    4) step: The current binary split step\n",
    "    5) tree_depth: The depth of the current tree\n",
    "    6) max_depth: Maximum depth this tree can grow\n",
    "    7) min_number: Minimum number of node size\n",
    "    8) min_infogain: Minimum information gain\n",
    "    9) counter: Indicate this is nth split\n",
    "    10) max_split: Maximum number of splits\n",
    "    \n",
    "    Outputs:\n",
    "    1) tree_nodes: Nested tree nodes, which are stored and shown in nested dictionary type    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # If samples are empty, return None\n",
    "    if samples is None or len(samples)==0:\n",
    "        return None\n",
    "           \n",
    "    \n",
    "    current_features = features # Current feature list\n",
    "    labels = samples[output] # Output labels in the current tree node\n",
    "\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"Step %s: Current tree depth is %s. Current tree node has %s data points\" % (step, tree_depth, len(samples)) # Sample size\n",
    "    \n",
    "    # Verify whether stopping conditions 1-4 are satisfied. If satisfied, return a leaf_node\n",
    "    if stop_1(labels) or stop_2(current_features) or stop_3(tree_depth, max_depth) or stop_4(samples, min_number):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None          \n",
    "            \n",
    "                }    \n",
    "    \n",
    "    \n",
    "    # If pass stopping conditions 1-4 , then do best splitting    \n",
    "    best_split = best_feature_split(samples, output, current_features)\n",
    "    best_feature, best_infogain, best_left, best_right = best_split[0], best_split[1], best_split[2], best_split[3]    \n",
    "    \n",
    "    # Verify whether stopping condition 5 is satisfied. If satisfied, return a leaf node\n",
    "    if stop_5(best_infogain, min_infogain):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None          \n",
    "            \n",
    "                }      \n",
    "    \n",
    "    \n",
    "    # If pass stopping condition 5, then move on \n",
    "    # If reach maximum number of splits\n",
    "    split_counter = counter.pop(0) # Indicate this is nth split\n",
    "    if split_counter >= max_split:\n",
    "        print \"Stopping Condition: You already reach maximum number of splits.\"\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None          \n",
    "            \n",
    "                }    \n",
    "    else:\n",
    "        step += 1\n",
    "        print \"Step %s: Binary split on %s. Size of Left and Right tree is (%s, %s)\" % \\\n",
    "              (step, best_feature, len(best_left) if best_left is not None else 0, len(best_right) if best_right is not None else 0)\n",
    "        current_features.remove(best_feature) # Remove this feature if this feature is used for split   \n",
    "        print \"Note: You have already done %s splits.\" % (split_counter)      \n",
    "    \n",
    "    \n",
    "       \n",
    "    # Do binary split on left tree and right tree in a recursive way\n",
    "    left_split = ClassificationTree(best_left, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain, \\\n",
    "                                    counter, max_split) \n",
    "    right_split = ClassificationTree(best_right, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain, \\\n",
    "                                     counter, max_split) \n",
    "    \n",
    "    return {\n",
    "            'label': None,\n",
    "            'left_tree': left_split,\n",
    "            'right_tree': right_split,\n",
    "            'best_feature': best_feature        \n",
    "            \n",
    "            }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose max_depth = 6; min_infogain=5e-4\n",
    "features = list(dataset_copy.columns[2:])\n",
    "output = dataset_copy.columns[1]\n",
    "maximum_split = 7\n",
    "counters = [x for x in range(1,101)]\n",
    "tree_model = ClassificationTree(dataset_copy, output, features, step=0, tree_depth=0, max_depth=10, min_number=5, min_infogain=-1, \\\n",
    "                                counter=counters, max_split=maximum_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree_model, depth=0, LR=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to predict output labels of data samples. <br/>\n",
    "\n",
    "Remember our tree structure is like: <br/>\n",
    "{ <br/>\n",
    "   'label': None for temporary node, or predicted label at the leaf node (e.g., \"Majority Voting\" criterion) for leaf node; <br/>\n",
    "   'left_tree': Left tree after the selected feature (=0 or -1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'right_tree': Right tree after the selected feature (=1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'best_feature': The feature that is selected to do binary split for temporary node, None for leaf node. <br/>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(new_sample, train_tree):   \n",
    "    '''This function is used to predict the label of one new sample.\n",
    "    Inputs:\n",
    "    1) new_sample: A new sample, we would like to predict its label (Pandas DataFrame)\n",
    "    2) train_tree: The classification tree we have just trained\n",
    "    \n",
    "    Outputs:\n",
    "    1) predict_label: The predicted label for this new sample  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # If move to the leaf node\n",
    "    if train_tree['best_feature']==None:\n",
    "        return train_tree['label']\n",
    "    \n",
    "    # If still stay at temporary node\n",
    "    else:\n",
    "        # Find the value of the best feature in the current node\n",
    "        # If value is 0, then go to left tree\n",
    "        # If value is 1, then go to right tree\n",
    "        # Remember what your have learned in Data Structure course, about binary tree\n",
    "        best_feature = train_tree['best_feature']\n",
    "        return predict_label(new_sample, train_tree['left_tree']) if new_sample[best_feature]==0 \\\n",
    "               else predict_label(new_sample, train_tree['right_tree'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to learn partial and apply function. They are powerful.\n",
    "from functools import partial\n",
    "prediction = partial(predict_label, train_tree=tree_model)\n",
    "predicted_labels = dataset_copy.apply(lambda x: prediction(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate predicted_labels into our dataset\n",
    "dataset_copy['prediction'] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignments: \n",
    "* Write functions to calculate `Gini index` and `misclassification error rate` metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "\n",
    "<img src='http://p3.pstatp.com/large/149b000695c099a83da7' width='400' height='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(sample_labels):\n",
    "    '''This function is used to calculate gini-index of a given tree node, in which there are samples with labels (0, 1) or (-1, 1).\n",
    "    Inputs:\n",
    "    1) sample_labels: Labels for samples in the current tree node, such as (1, 0, 0, 1, 0) or (1, -1, -1, 1, 0)\n",
    "    \n",
    "    Outputs:\n",
    "    1) gini: gini-index of labels in the current tree node.       \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Assert np.array\n",
    "    sample_labels = np.array(sample_labels)\n",
    "    \n",
    "    # What if sample_labels are empty\n",
    "    if sample_labels.size == 0:\n",
    "        return 0  \n",
    "    \n",
    "    # What if all the labels are the same\n",
    "    class_values = np.unique(sample_labels) # Sample labels/classes; Usually (0,1), sometimes (-1,1)\n",
    "    num0 = len(filter(lambda x:x==class_values[0], sample_labels)) # Number of samples with one label\n",
    "    num1 = len(filter(lambda x:x==class_values[1], sample_labels)) if class_values.size > 1 else 0 # Number of samples with another label\n",
    "    \n",
    "    if sample_labels.size == num0 or sample_labels.size == num1:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate entropy value      \n",
    "    p0 = num0 / (num0+num1) # Probability of class 0 labels\n",
    "    p1 = 1 - p0 # Probability of class 1 labels\n",
    "    \n",
    "    gini = 1 - (p0*p0+p1*p1)    \n",
    "    \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_error(sample_labels):\n",
    "    '''This function is used to calculate misclassification error (rate) of a given tree node, \n",
    "       in which there are samples with labels (0, 1) or (-1, 1).\n",
    "       \n",
    "    Inputs:\n",
    "    1) sample_labels: Labels for samples in the current tree node, such as (1, 0, 0, 1, 0) or (1, -1, -1, 1, 0)\n",
    "    \n",
    "    Outputs:\n",
    "    1) error: misclassification error of labels in the current tree node.       \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Assert np.array\n",
    "    sample_labels = np.array(sample_labels)\n",
    "    \n",
    "    # What if sample_labels are empty\n",
    "    if sample_labels.size == 0:\n",
    "        return 0  \n",
    "    \n",
    "    # What if all the labels are the same\n",
    "    class_values = np.unique(sample_labels) # Sample labels/classes; Usually (0,1), sometimes (-1,1)\n",
    "    num0 = len(filter(lambda x:x==class_values[0], sample_labels)) # Number of samples with one label\n",
    "    num1 = len(filter(lambda x:x==class_values[1], sample_labels)) if class_values.size > 1 else 0 # Number of samples with another label    \n",
    "       \n",
    "    # Calculate misclassification error \n",
    "    error = num0 if num1>=num0 else num1  \n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(samples, output, feature):\n",
    "    '''This function is used to calculate misclassification error rate when a given tree node is splitted by a given feature.\n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    1) output: Name of the output column\n",
    "    2) feature: Name of the feature used to split the current tree node. Remember the features we selected in this case are binary.\n",
    "    \n",
    "    Outputs:\n",
    "    1) error_rate: The misclassification error rate if the current tree node is splitted by the feature\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Split samples by feature values into subsamples\n",
    "    subsamples = defaultdict()\n",
    "    N_errors = 0 # Initialize total misclassification errors\n",
    "    \n",
    "    for feature_value in np.unique(samples[feature]):\n",
    "        subsamples[feature_value] = samples[samples[feature] == feature_value]\n",
    "        temp = subsamples[feature_value] # Store a temporary copy\n",
    "        N_errors += misclassification_error(temp[output]) # Total misclassification errors\n",
    "                \n",
    "    # Calculate error rate\n",
    "    error_rate = N_errors / float(len(samples))\n",
    "    \n",
    "    return error_rate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "a = np.array([[1,0,0,1],[0,1,1,0],[1,1,1,1],[0,0,0,0],[1,1,0,0]])\n",
    "data = pd.DataFrame(a, columns=['x1','x2','x3','y'])\n",
    "#best_feature_split(data, 'y', ['x1','x2','x3'])[0]\n",
    "print \"Gini index:\", gini_index(data['y'])\n",
    "print \"Misclassification error:\", misclassification_error(data['y'])\n",
    "print \"Misclassification error rate of one split:\", error_rate(data, 'y', 'x1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignments: Post-pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See textbook Chapter 9.4 pp.130\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to randomly split the original dataset into training dataset (used for training model) and pruning dataset (used for post-pruning). \n",
    "\n",
    "```python\n",
    "# Suppose 70% for training model, 30% for pruning.\n",
    "# Remember you need to set seed first, otherwise you will get different training data and pruning data. Then you will get different \n",
    "# training model, and different results from pruning model\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "import random\n",
    "\n",
    "random.seed(12345)\n",
    "\n",
    "new_dataset = copy.deepcopy(dataset_copy)\n",
    "features = list(dataset.columns[2:])\n",
    "output = dataset.columns[1]\n",
    "num_rows = new_dataset.shape[0]\n",
    "\n",
    "new_dataset = shuffle(new_dataset, random_state=12345) # Randomly shuffle the dataset\n",
    "\n",
    "train_data = new_dataset.iloc[:int(num_rows*0.7),:]\n",
    "prune_data = new_dataset.iloc[int(num_rows*0.7):,:]\n",
    "\n",
    "print train_data.shape[0], prune_data.shape[0]\n",
    "\n",
    "assert train_data.shape[0]+prune_data.shape[0] == new_dataset.shape[0]\n",
    "\n",
    "#Store a copy: Export to csv file\n",
    "train_data.to_csv('./traindata_postpruning.csv', index=None)\n",
    "prune_data.to_csv('./prunedata_postpruning.csv', index=None)\n",
    "print train_data[output].value_counts()\n",
    "print prune_data[output].value_counts()\n",
    "```\n",
    "\n",
    "## In order to ensure that you get the same post-pruning results, you only need to import training data and pruning data that are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training data and pruning data that are provided\n",
    "train_data=pd.read_csv('./traindata_postpruning.csv')\n",
    "prune_data=pd.read_csv('./prunedata_postpruning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefine the tree function and tree structure:\n",
    "Remember we plan to calculate error rate of each node. <br/>\n",
    "\n",
    "Revise our tree structure is like: <br/>\n",
    "{ <br/>\n",
    "   'label': Predicted label at the current node (e.g., \"Majority Voting\" criterion) for leaf node; <br/>\n",
    "   'left_tree': Left tree after the selected feature (=0 or -1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'right_tree': Right tree after the selected feature (=1) is splitted for temporary node, None for leaf node; <br/>\n",
    "   'best_feature': The feature that is selected to do binary split for temporary node, None for leaf node; <br/>\n",
    "   'error_rate': error rate of this node; calculated using pruning dataset; Need initialization; <br/>\n",
    "   'depth': How deep is this node; <br/>\n",
    "   'prune_data_labels': A list of actual labels of pruning dataset; <br/>\n",
    "   'N_prune_data': Number of pruning data samples in current node. <br/>\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](image/1.png)\n",
    "![title](image/2.png)\n",
    "![title](image/3.png)\n",
    "![title](image/4.png)\n",
    "![title](image/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the tree function: Add 3 more keys in node dictionary, for conveniently calculating prediction error rate of each node.\n",
    "# Now the intermediate node should also have predicted output label (using majority voting)\n",
    "\n",
    "def ClassificationTree(samples, output, features, step, tree_depth, max_depth, min_number, min_infogain, counter, max_split):\n",
    "    '''This function is used to build a classification tree in a recursive way.\n",
    "       Remember how you build a binary tree in the previous C++ and Data Structure courses).\n",
    "       \n",
    "    Inputs:\n",
    "    1) samples: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) features: A list of feature names\n",
    "    4) step: The current binary split step\n",
    "    5) tree_depth: The depth of the current tree\n",
    "    6) max_depth: Maximum depth this tree can grow\n",
    "    7) min_number: Minimum number of node size\n",
    "    8) min_infogain: Minimum information gain\n",
    "    9) counter: Indicate this is nth split\n",
    "    10) max_split: Maximum number of splits\n",
    "    \n",
    "    Outputs:\n",
    "    1) tree_nodes: Nested tree nodes, which are stored and shown in nested dictionary type    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # If samples are empty, return None\n",
    "    if samples is None or len(samples)==0:\n",
    "        return None\n",
    "           \n",
    "    \n",
    "    current_features = features # Current feature list\n",
    "    labels = samples[output] # Output labels in the current tree node\n",
    "\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"----------------------------------------------------------------------------\"\n",
    "    print \"Step %s: Current tree depth is %s. Current tree node has %s data points\" % (step, tree_depth, len(samples)) # Sample size\n",
    "    \n",
    "    # Verify whether stopping conditions 1-4 are satisfied. If satisfied, return a leaf_node\n",
    "    if stop_1(labels) or stop_2(current_features) or stop_3(tree_depth, max_depth) or stop_4(samples, min_number):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None,\n",
    "                'error_rate': -float('inf'),\n",
    "                'depth': tree_depth,\n",
    "                'prune_data_labels': [],\n",
    "                'N_prune_data': 0\n",
    "            \n",
    "                }    \n",
    "    \n",
    "    \n",
    "    # If pass stopping conditions 1-4 , then do best splitting    \n",
    "    best_split = best_feature_split(samples, output, current_features)\n",
    "    best_feature, best_infogain, best_left, best_right = best_split[0], best_split[1], best_split[2], best_split[3]    \n",
    "    \n",
    "    # Verify whether stopping condition 5 is satisfied. If satisfied, return a leaf node\n",
    "    if stop_5(best_infogain, min_infogain):\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None,\n",
    "                'error_rate': -float('inf'),\n",
    "                'depth': tree_depth,\n",
    "                'prune_data_labels': [],\n",
    "                'N_prune_data': 0\n",
    "            \n",
    "                }      \n",
    "    \n",
    "    \n",
    "    # If pass stopping condition 5, then move on \n",
    "    # If reach maximum number of splits\n",
    "    split_counter = counter.pop(0) # Indicate this is nth split\n",
    "    if split_counter >= max_split:\n",
    "        print \"Stopping Condition: You already reach maximum number of splits.\"\n",
    "        return {\n",
    "                'label': majority_vote(labels),\n",
    "                'left_tree': None,\n",
    "                'right_tree': None,\n",
    "                'best_feature': None,\n",
    "                'error_rate': -float('inf'),\n",
    "                'depth': tree_depth,\n",
    "                'prune_data_labels': [],\n",
    "                'N_prune_data': 0\n",
    "            \n",
    "                }    \n",
    "    else:\n",
    "        step += 1\n",
    "        print \"Step %s: Binary split on %s. Size of Left and Right tree is (%s, %s)\" % \\\n",
    "              (step, best_feature, len(best_left) if best_left is not None else 0, len(best_right) if best_right is not None else 0)\n",
    "        current_features.remove(best_feature) # Remove this feature if this feature is used for split   \n",
    "        print \"Note: You have already done %s splits.\" % (split_counter)      \n",
    "    \n",
    "    \n",
    "       \n",
    "    # Do binary split on left tree and right tree in a recursive way\n",
    "    left_split = ClassificationTree(best_left, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain, \\\n",
    "                                    counter, max_split) \n",
    "    right_split = ClassificationTree(best_right, output, current_features, step+1, tree_depth+1, max_depth, min_number, min_infogain, \\\n",
    "                                     counter, max_split) \n",
    "    \n",
    "    return {\n",
    "            'label': majority_vote(labels),# Now the intermediate node should also have predicted output label (using majority voting)\n",
    "            'left_tree': left_split,\n",
    "            'right_tree': right_split,\n",
    "            'best_feature': best_feature,\n",
    "            'error_rate': float('inf'),\n",
    "            'depth': tree_depth,\n",
    "            'prune_data_labels': [],\n",
    "            'N_prune_data': 0\n",
    "            \n",
    "            }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pruning dataset to get error_rate of each node\n",
    "# Each pruning data sample should traverse the train_model, store actual labels of pruning data into respective nodes \n",
    "features = list(dataset.columns[2:])\n",
    "output = dataset.columns[1]\n",
    "\n",
    "def traverse(new_sample, train_tree, output_name):   \n",
    "    '''This function is used to traverse the train tree model of each sample in pruning data, \n",
    "       and update the node values in the train tree model.\n",
    "       \n",
    "    Inputs:\n",
    "    1) new_sample: A new sample, we would like to predict its label (Pandas DataFrame)\n",
    "    2) train_tree: The classification tree we have just trained\n",
    "    3) output_name: output variable name of this new_sample\n",
    "    \n",
    "    Outputs:\n",
    "    1) Update the tree node values of the training tree model we have obtained, such as 'prune_data_labels' and 'error_rate'\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Store this actual label into this node\n",
    "    train_tree['prune_data_labels'].append(new_sample[output_name]) \n",
    "    \n",
    "    # Update current error_rate\n",
    "    temp = train_tree['prune_data_labels']\n",
    "    train_tree['error_rate'] = len(filter(lambda x: x != train_tree['label'], temp)) / len(temp)\n",
    "    \n",
    "    # Update the number of pruning data samples in the current tree node\n",
    "    train_tree['N_prune_data'] = len(temp)\n",
    "    \n",
    "    # If in the leaf node, then stop\n",
    "    if train_tree['best_feature']==None:\n",
    "        return       \n",
    "    \n",
    "    # If in a temporary node, then need further traverse\n",
    "    else:\n",
    "        # Find the value of the best feature in the current node\n",
    "        # If value is 0, then go to left tree\n",
    "        # If value is 1, then go to right tree        \n",
    "        best_feature = train_tree['best_feature']\n",
    "        traverse(new_sample, train_tree['left_tree'], output_name) if new_sample[best_feature]==0 \\\n",
    "               else traverse(new_sample, train_tree['right_tree'], output_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pruning dataset to get error_rate of each node\n",
    "# Each pruning data sample should traverse the train_model, store actual labels of pruning data into respective nodes \n",
    "features = list(dataset.columns[2:])\n",
    "output = dataset.columns[1]\n",
    "maximum_split = 20\n",
    "counters = [x for x in range(1,101)]\n",
    "train_model = ClassificationTree(train_data, output, features, step=0, tree_depth=0, max_depth=4, min_number=5, min_infogain=5e-4, \\\n",
    "                                counter=counters, max_split=maximum_split)\n",
    "\n",
    "for i in range(0, len(prune_data)):\n",
    "    row = prune_data.iloc[i]\n",
    "    traverse(row, train_model, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-pruning the training tree model: From bottom to up\n",
    "# Recursively pruning on the branch that the parent node's left+right child nodes are both leaf nodes\n",
    "import copy\n",
    "\n",
    "# Make a deepcopy of the original model\n",
    "# Please note the difference between shallow copy and deep copy in Python\n",
    "prune_model = copy.deepcopy(train_model)\n",
    "\n",
    "# Post-pruning\n",
    "# Idea: Find the father node whose left and right child are both leaf nodes -> Compare parent node's error rate with weighted error rate -> \n",
    "#       Repeat until nothing to do\n",
    "def post_prune(train_tree):   \n",
    "    '''This function is used to do post-pruning for one tree model.\n",
    "       \n",
    "    Inputs:\n",
    "    1) train_tree: The classification tree we have just trained\n",
    "        \n",
    "    Outputs:\n",
    "    1) Update the train_tree\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # In the current node, if left and right child are both leaf nodes, then decide whether to prune\n",
    "    if train_tree['left_tree']['best_feature'] == None and train_tree['right_tree']['best_feature'] == None:\n",
    "        p_left = train_tree['left_tree']['N_prune_data'] / train_tree['N_prune_data'] # proportion of samples in the left leaf node\n",
    "        p_right = train_tree['right_tree']['N_prune_data'] / train_tree['N_prune_data'] # proportion of samples in the right leaf node\n",
    "        weight_error = p_left * train_tree['left_tree']['error_rate'] + p_right * train_tree['right_tree']['error_rate'] # weighted error rate\n",
    "        print train_tree['best_feature'], ':', 'Father node error_rate:', train_tree['error_rate'], ',', 'Weighted error_rate:', weight_error\n",
    "        \n",
    "        # If weighted error_rate of left+right child nodes is higher than error_rate of parent node, then do pruning\n",
    "        if weight_error >= train_tree['error_rate']:\n",
    "            train_tree['left_tree'], train_tree['right_tree'], train_tree['best_feature'] = None, None, None\n",
    "        return\n",
    "        \n",
    "    else:\n",
    "        # Pruning left subtree and then right subtree in a recursive way\n",
    "        # In the current node, if left subtree or right subtree is None (i.e., current node is leaf node), then pass\n",
    "        try: \n",
    "            post_prune(train_tree['left_tree'])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            post_prune(train_tree['right_tree']) \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        \n",
    "post_prune(prune_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do post-pruning multiple times, until the tree structure does not change\n",
    "flag = True\n",
    "while flag:\n",
    "    old = copy.deepcopy(prune_model)\n",
    "    post_prune(prune_model)\n",
    "    if prune_model == old:\n",
    "        flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the tree model before and after post-pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree structure before pruning\n",
    "print_tree(train_model, depth=0, LR=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree structure after pruning\n",
    "print_tree(prune_model, depth=0, LR=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that some candidate branches are pruned. For each candidate branch, after post-pruning it, the parent node becomes leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Open-Source Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a break and let us use open-source package to run decision tree models. <br/>\n",
    "Use `Scikit-learn`to make classification trees and make predictions: http://scikit-learn.org/stable/modules/tree.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update our features and output\n",
    "features = list(dataset.columns[2:])\n",
    "output = dataset.columns[1]\n",
    "\n",
    "# Split dataset to do validation\n",
    "X = dataset[features]\n",
    "y = dataset[output]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on train data\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree_model = decision_tree.fit(X_train, y_train)\n",
    "decision_tree_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted labels for test data\n",
    "y_pred = decision_tree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print cm\n",
    "print TN, FP, FN, TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of decision tree model\n",
    "print \"Accuracy: \", accuracy_score(y_test, y_pred)\n",
    "print \"Sensitivity: \", recall_score(y_test, y_pred)\n",
    "print \"Precision: \", precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate:\n",
    "1. Accuracy\n",
    "2. Misclassification rate\n",
    "3. Precision\n",
    "4. Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get predicted scores Pr(y=1): Used as thresholds for calculating TP Rate and FP Rate\n",
    "score = decision_tree_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, score) # fpr: FP Rate, tpr: TP Rate, thresholds: Pr(y=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree\n",
    "# Remember you should install package graphviz first\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(decision_tree_model, out_file=None, feature_names=features, class_names=output, \n",
    "                                filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in .pdf \n",
    "graph.render(\"Lending Club Loan Status\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Assignments: \n",
    "* Use Titanic data (“train.csv”); Fit the model using `scikit-learn` with different metrics (e.g., information gain, gini index)\n",
    "* Observe and report the differences (e.g., best features for splitting, tree structure, performance, etc.)\n",
    "* There are no right or wrong answers. Don't worry. Just report what you've seen. \n",
    "\n",
    "**Note:** You may need to do simple data cleaning by yourself, such as binarizing output variable \"survived\", and transforming categorical variables to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic dataset\n",
    "%pwd\n",
    "titanic = pd.read_csv(\"./train.csv\", low_memory=False)\n",
    "titanic.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform categorical features to binary features\n",
    "age_dummy = pd.get_dummies(titanic['Age Class'], prefix='Ageclass')  \n",
    "passenger_dummy = pd.get_dummies(titanic['Passenger Class'], prefix='passenger')\n",
    "titanic['output'] = titanic['Survived'].map(lambda x: 1 if x == 'Yes' else 0)\n",
    "titanic['gender'] = titanic['Gender'].map(lambda x: 1 if x == 'Male' else 0)\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and output\n",
    "features = ['Age']       \n",
    "output = ['output']\n",
    "\n",
    "dataset = titanic[features+output]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.join([age_dummy, passenger_dummy])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update our features and output\n",
    "features = dataset.columns[2:]\n",
    "output = dataset.columns[1]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on train data\n",
    "X = dataset[features]\n",
    "y = dataset[output]\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "decision_tree_model = decision_tree.fit(X, y)\n",
    "decision_tree_model.classes_\n",
    "\n",
    "# You can also do cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(decision_tree_model, out_file=None, feature_names=features, class_names=output, \n",
    "                                filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on train data\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini')\n",
    "decision_tree_model = decision_tree.fit(X, y)\n",
    "\n",
    "dot_data = tree.export_graphviz(decision_tree_model, out_file=None, feature_names=features, class_names=output, \n",
    "                                filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Questions (Just think about them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 What if features are continuous?\n",
    "\n",
    "* Please refer to text book chapter 8 pp.93\n",
    "\n",
    "### 5.2 What if output is continuous? \n",
    "\n",
    "* Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 References\n",
    "\n",
    "[1] Jason Brownlee, 2018, [Machine Learning Algorithms from Scratch with Python](https://machinelearningmastery.com/machine-learning-algorithms-from-scratch/). <br/>\n",
    "[2] Peter Harrington, 2012. Machine Learning in Action. Shelter Island, NY: Manning Publications Co."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
