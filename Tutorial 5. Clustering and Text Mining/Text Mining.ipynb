{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Introduction to Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal\n",
    "\n",
    "In this notebook, we will explore Text Mining including:\n",
    "* Handling text data\n",
    "* Parsing and Removing Punctuation\n",
    "* Tokenization\n",
    "* tf-idf\n",
    "\n",
    "The examples shown in this notebook are based on [\"Introduction to Machine Learning with Python: A Guide for Data Scientists\"](#3-References) and [\"Machine Learning with Python Cookbook\"](#3-References). The codes here are revised and different from the original ones.\n",
    "\n",
    "A typical text mining procedure:\n",
    "* Extracting keywords from text\n",
    "* Preprocessing: Converting unstructured data to structured data\n",
    "* Keywords selection\n",
    "* Clustering: Group similar words together; Identify common patterns/styles\n",
    "* Analysis: Identify relationships between the information in text and the focal outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division\n",
    "from math import sqrt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Understand Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four data types that you frequently encounter:\n",
    "* Numerical data\n",
    "* Categorical data\n",
    "* Structured String data\n",
    "* Text data\n",
    "\n",
    "The first three are all structured data. In this notebook, you will learn how to handle text data. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cleaning Text\n",
    "Some simple work on cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have a text data and you already convert it to structured type\n",
    "text_data = [\"      This is a BT2101 course    .\",\n",
    "            \"This course teaches Machine learning methods,\",\n",
    "            \"    including Supervised, unsupervised, and Deep learning models.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some simple preparation work\n",
    "\n",
    "# 1. Strip whitespace\n",
    "strip_whitespace = map(lambda x:x.strip(), text_data)\n",
    "strip_whitespace\n",
    "\n",
    "# 2. Remove periods\n",
    "remove_periods = map(lambda y:y.replace(\".\",\"\"), strip_whitespace)\n",
    "remove_periods\n",
    "\n",
    "# 3. Captitalize\n",
    "capital_initial = map(lambda z:z.upper(), remove_periods)\n",
    "capital_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing is the process of structuring the input text and deriving patterns within the structured data, including:\n",
    "* Sentence Segmentation\n",
    "* Remove stop words (e.g., numbers, puctuations, symbols, whitespace)\n",
    "* Tokenization\n",
    "* Stemming (Text Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words\n",
    "Removing the words that are very commonly used (but less informative) in a given language, we can focus on the important words instead.\n",
    "* Articles (the, a, an…)\n",
    "* Prepositions (for, after, above, across, before, under…)\n",
    "* Conjunctions (and, but, nor, yet, so, than…)\n",
    "* Pronouns (she, he, I, you, they, them…)\n",
    "* Auxiliary Verbs (can, will, could, would, must) and Linking Verbs (is, are, am)\n",
    "* When, where, how, what, which\n",
    "* Punctuations\n",
    "\n",
    "Stop words are the common words that is used in the language (e.g., a, the, so, them, he, she, who, what, when, how, is, are, etc.). In text processing, stop words are usually ignored to improve performance (speed & accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Remove punctuations\n",
    "import string\n",
    "import re\n",
    "\n",
    "remove_punctuation = map(lambda x:x.translate(None, string.punctuation), text_data)\n",
    "remove_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Remove words that are very commonly used but less informative\n",
    "# Load library (Natural Language Toolkit NLTK)\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are these stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have a list of words, and you want to remove stop words from them\n",
    "strings = \"i am going to take this BT2101 module because it is very very interesting\"\n",
    "word_list = strings.split()\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "word_list_without_stopwords = [word for word in word_list if word not in stop_words]\n",
    "word_list_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "A token is a meaningful unit of text, most often a word, that we are interested in using for further analysis. Tokenization is the process of splitting text into tokens (i.e., individual words). \n",
    "* Generally an easy task for English\n",
    "* Split the string by space and punctuation\n",
    "* Some problems for hypenation, apostrophe, periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library (Natural Language Toolkit NLTK)\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have a text\n",
    "text = remove_punctuation[0] + remove_punctuation[1] + remove_punctuation[2]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize into words\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize into sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"This is a BT2101 course. This course teaches Machine learning methods. You will learn Supervised, unsupervised, and Deep learning models.\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Text Normalization\n",
    "Text Normalization is a process of transforming text into a form that is consistent. For example:\n",
    "* I’m really HAPPY! → i’m really happy!\n",
    "* U.S.A → usa\n",
    "* café → cafe\n",
    "\n",
    "The purpose of stemming is to make the text more “general”, so that \"café\" and \"cafe\" are treated the same manner. \n",
    "\n",
    "In additional to the obvious transformation (change to lower case, etc), we can also transform words to their stem (or root form):\n",
    "* books → book\n",
    "* beautiful → beauty\n",
    "* eats → eat\n",
    "\n",
    "This process is called Stemming. Porter stemmer is a popular rule-based stemming algorithm:\n",
    "1. Remove plurals, -ed, -ing\n",
    "2. Turn terminal y to i when there is another vowel in the stem: (e.g., furry → furri, fry → fry)\n",
    "3. Maps double suffixes to single ones (e.g., playfulness →playful)\n",
    "4. Deals with suffixes, -full, -ness, etc. \n",
    "5. Takes off –ant, -ence, etc.\n",
    "6. Removes the final -e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use NLTK's PorterStemmer to do stemming\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have a word tokens, and you want to do stemming on it\n",
    "strings = \"i am interested in these amazing machine learning models\"\n",
    "tokenized_words = strings.split()\n",
    "tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do porter stemming\n",
    "porter = PorterStemmer()\n",
    "porter_stem = map(lambda x: porter.stem(x), tokenized_words)\n",
    "porter_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common methods of transforming text into features is by using a **bag-of-words** model. Bag-of-words models output a feature for every unique word in text data, with each feature containing a count of occurences in observations. For example, in our solution the sentence `I love Brazil. Brazil!` has a value of 2 in the `brazil` feature, because the word *brazil* appears twice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to create a set of features indicating the number of times an observation's text contains a particular word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text\n",
    "text_data = np.array(['I love machine learning. Machine learning!', 'Ensemble learning is the best', 'Deep learning beats all'])\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag of words\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagofwords = pd.DataFrame(bag_of_words.toarray(), columns=count.get_feature_names())\n",
    "bagofwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have weighted each token based on its term frequency (frequency of occurrence in the doc)\n",
    "* Idea: words that occur more → document seems to focus more on that idea\n",
    "* However, (after excluding the stop words), some words are more common than the others\n",
    "* Does not necessarily mean that they are more important than other low frequency words\n",
    "\n",
    "An improvement is to also consider how a word is used in other documents in the corpus. The statistic **tf-idf** is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.\n",
    "\n",
    "tf-idf (term frequency * inverse document frequency) is an approach to reflect how important a word is.\n",
    "* Made up of 2 components: tf and idf\n",
    "* tf = how many times the term appears in the document\n",
    "\n",
    "<img src=\"http://dovgalecs.com/blog/wp-content/uploads/2012/03/img131.gif\" width=\"500\">\n",
    "\n",
    "Example, if we have 100 documents in our corpus, and the term “SOC” appears in just 1 document $idf(“soc”) = log (100)$\n",
    "* The term “i” (stop word) appears in all the documents $idf(“i”) = log(1) = 0$\n",
    "* Justification for idf\n",
    "\n",
    "If a term appears on many documents, each time it appears in a document probably not important. If a term is seldom seen, when it appears likely to be important (i.e. document is likely to be about it).\n",
    "\n",
    "After motivating why idf makes sense, we still need to cater for the fact that, if a term is mentioned many times in a document, it is probably important: $Final weight = tf * idf$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text\n",
    "text_data = np.array(['I love machine learning. Machine learning!', 'Ensemble learning is the best', 'Deep learning beats all'])\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf feature matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = pd.DataFrame(feature_matrix.toarray(), columns=tfidf.vocabulary_.keys())\n",
    "tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about `NLTK` can be found at https://www.nltk.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 References\n",
    "[1] Müller, A.C. and Guido, S., 2016. Introduction to Machine Learning with Python: A Guide for Data Scientists. O'Reilly Media, Inc. <br/>\n",
    "[2] Chris Albon. (2018). Machine Learning with Python Cookbook. O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
