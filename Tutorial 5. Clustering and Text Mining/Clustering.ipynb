{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 K-Means and Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal\n",
    "\n",
    "In this notebook, we will explore clustering method, an unsupervised learning method, including:\n",
    "* K-Means Clustering\n",
    "* Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division\n",
    "from math import sqrt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 K-Means Clusting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means Clustering steps:\n",
    "\n",
    "1. Determine the number of k (i.e., number of clusters)\n",
    "2. Randomly select k centroids (i.e. centers of clusters)\n",
    "3. Assign each data point to its closest centroid\n",
    "4. Recalculate the centroids as the average of all data points in a cluster\n",
    "5. Assign data points to their closest centroids\n",
    "6. Repeat Step 4 and 5 until the observations are not reassigned or the maximum number of iterations is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit k-means clustering model: Suppose 3 clusters\n",
    "cluster_3 = KMeans(n_clusters=3, random_state=0, n_jobs=-1)\n",
    "model_3 = cluster_3.fit(features)\n",
    "model_std_3 = cluster_3.fit(features_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View cluster centers for each cluster:\n",
    "model_std_3.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View predicted clusters:\n",
    "model_std_3.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with true classes:\n",
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict which cluster is a new observation in:\n",
    "new_obs = [[0.5, 0.5, 0.5, 0.5]]\n",
    "model_std_3.predict(new_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose k? \n",
    "#### Elbow method\n",
    "* Gauge how the heterogeneity within clusters changes for various of k.\n",
    "* The heterogeneity within clusters is expected to decreases with more clusters.\n",
    "* The heterogeneity is measured by within-clusters/groups sum of squares (WSS)\n",
    "\n",
    "\\begin{align}\n",
    "WSS(k) = \\mathop{\\sum}_{i=1}^{N}\\mathop{\\sum}_{j=0}^{p}(x_{ij}-centroid(x_{kj}))^2\n",
    "\\end{align}\n",
    "\n",
    "Suppose N observations and p features. <br/>\n",
    "k is cluster id ($=1,...,K$). <br/>\n",
    "$x_{ij}$ is $i^{th}$ observation $j^{th}$ feature. <br/>\n",
    "$centroid(x_{kj})$ is $k^{th}$ centroid of feature $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scipy used for calculate distances\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster data with 1-10 clusters and get centroids\n",
    "\n",
    "N_cluster = range(1, 11) # 1 to 10 clusters\n",
    "WSS_list = [] # A list of WSS for 1 to 10 clusters\n",
    "\n",
    "for k in N_cluster:\n",
    "    \n",
    "    # Run k-means model\n",
    "    cluster = KMeans(n_clusters=k, random_state=0, n_jobs=-1) # Run the model: Assume k clusters\n",
    "    model = cluster.fit(features_std) # Fit the standardized data\n",
    "    centroids = model.cluster_centers_ # Get centroids for each cluster id (=0,...,k-1)\n",
    "    labels = model.labels_ # Get labels (cluster id) for each observation\n",
    "    \n",
    "    # Calculate WSS(k)\n",
    "    squared_distance = cdist(features_std, centroids, 'sqeuclidean') # Calculate squared distance between observations and centroids\n",
    "    min_distance_cluster_id = np.argmin(squared_distance, axis=1) # Find index of minimum squared distance for each observation\n",
    "    min_distance = np.min(squared_distance, axis=1) # Find minimum squared distance for each observation\n",
    "    WSS = np.sum(min_distance)\n",
    "    \n",
    "    WSS_list.append(WSS)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show WSS for 1-10 clusters\n",
    "WSS_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow curve\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(N_cluster, WSS_list, 'g*-')\n",
    "ax.plot(N_cluster[1], WSS_list[1], marker='o', markersize=12, markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "ax.plot(N_cluster[2], WSS_list[2], marker='o', markersize=12, markeredgewidth=2, markeredgecolor='b', markerfacecolor='None')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Within-cluster Sum of Squares (WSS)')\n",
    "plt.title('Elbow curve for K-means clustering')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Which number of clusters do you prefer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original code can be found at http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Clustering Steps:\n",
    "\n",
    "1. Start with n clusters (each record is its own cluster)\n",
    "2. Merge two closest records into one cluster\n",
    "3. At each successive step, the two clusters closest to each other are merged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meanshift object\n",
    "HC = AgglomerativeClustering(n_clusters=3)\n",
    "HC_model = HC.fit(features_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show clusters\n",
    "HC_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "avg_link = linkage(features_std[1:20, :], method='average') # Linkage criterion: average distance\n",
    "dendrogram(avg_link).items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How to decide the number of clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about hierarchical clustering can be found at: <br/>\n",
    "[1] http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html <br/>\n",
    "[2] https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 References\n",
    "[1] Raschka, S. (2015). Python machine learning. Packt Publishing Ltd. <br/>\n",
    "[2] Chris Albon. (2018). Machine Learning with Python Cookbook. O'Reilly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
