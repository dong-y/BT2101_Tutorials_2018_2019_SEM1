{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Introduction to Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal:\n",
    "\n",
    "In this notebook, we will explore Naive Bayes classification using:\n",
    "* Maximize a-posterior probability (MAP)\n",
    "* Open-source package: `scikit-learn`\n",
    "\n",
    "For MAP, you will:\n",
    "* Understand Bayes formula: Likelihood, Prior, Posterior\n",
    "* Understand total/complete probability and conditional probability\n",
    "* Write functions to calculate likelihood, prior and posterior\n",
    "* Write a prediction function\n",
    "* Use MAP to make classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division\n",
    "from math import sqrt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Summary of Bayes Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture class, we know that a typical Bayes formula with regard to an event $\\theta$ is:\n",
    "\\begin{align}\n",
    "P(\\theta|X) &= \\frac{P(X|\\theta)P(\\theta)}{P(X)} \\\\\n",
    "&\\propto P(X|\\theta)P(\\theta)  \\\\\n",
    "\\end{align}\n",
    "\n",
    "which is equivalently expressed as:\n",
    "\\begin{align}\n",
    "\\text{Posterior} &= \\frac{\\text{Likelihood}\\times\\text{Prior}}{\\text{Evidence}} \\\\\n",
    "&\\propto \\text{Likelihood}\\times\\text{Prior}  \\\\\n",
    "\\end{align}\n",
    "\n",
    "Let us suppose:\n",
    "1. Prior probability distribution of an event $\\theta$ is: $P(\\theta)$\n",
    "\n",
    "2. Data likelihood function of $X=(x_{1},...,x_{f})$ with $f$ features:\n",
    "\\begin{align}\n",
    "l(\\theta) &= P(X=(x_{1},...,x_{f})|\\theta) \\\\\n",
    "&= \\prod_{j=1}^{f}P(x_{j}|\\theta)\n",
    "\\end{align}\n",
    "\n",
    "3. Posterior probability is: $P(\\theta|X=(x_{1},...,x_{f}))$\n",
    "\n",
    "4. According to Bayes formula, we can get:\n",
    "\\begin{align}\n",
    "P(\\theta|X) &\\propto P(\\theta)\\times\\prod_{j=1}^{f}P(x_{j}|\\theta) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Assumption: Features are independent, which is a very strong assumption !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 A Typical Naive Bayes Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na√Øve) independence assumptions between the features [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\n",
    "\n",
    "Suppose output $y$ is assigned to one of the classes $C_{1},...,C_{k}$, then the Naive Bayes classification problem is to **Maximize** the posterior probability:\n",
    "\n",
    "\\begin{align}\n",
    "y &= \\mathop{\\arg\\max}_{k\\in{1,...,K}}P(C_{k}|X) \\\\\n",
    "&= \\mathop{\\arg\\max}_{k\\in{1,...,K}}P(C_{k})\\prod_{j=1}^{f}P(x_{j}|C_{k}) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation has two important arguments:\n",
    "1. Prior probability of class $C_{k}$: $P(C_{k})$ for $k=1,...,K$\n",
    "2. Conditional probability of each feature $j$: $P(x_{j}|C_{k})$ for $j=1,...,f$\n",
    "\n",
    "Given by the data, we need to calcuate (1) prior probability of each class, and (2) probability of one feature conditional on one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this example can be found [here](http://www.inf.u-szeged.hu/~ormandi/ai2/06-naiveBayes-example.pdf).\n",
    "\n",
    "Attributes information:\n",
    "1. Color: Car colour\n",
    "2. Type: Car type\n",
    "3. Origin: Whether car is manufactured from domestic or imported\n",
    "4. Stolen: Whether car is stolen or not (1=Yes; 0=No)\n",
    "\n",
    "We aim to predict whether a (Red, Domestic, SUV) car will be stolen or not. <br/>\n",
    "\n",
    "So the problem becomes: <br/>\n",
    "Comparing $ P(Color=Red | Stolen=1)P(Type=SUV | Stolen=1)P(Origin=Domestic | Stolen=1)P(Stolen=1) $ <br/>\n",
    "with $ P(Color=Red | Stolen=0)P(Type=SUV | Stolen=0)P(Origin=Domestic | Stolen=0)P(Stolen=0) $. <br/>\n",
    "\n",
    "Your need to calculate these conditional probabilities and total probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create this dataset\n",
    "Color = ['Red','Red','Red','Yellow','Yellow','Yellow','Yellow','Yellow','Red','Red']\n",
    "Type = ['Sports','Sports','Sports','Sports','Sports','SUV','SUV','SUV','SUV','Sports']\n",
    "Origin = ['Domestic','Domestic','Domestic','Domestic','Imported','Imported','Imported','Domestic','Imported','Imported']\n",
    "Stolen = ['Yes','No','Yes','No','Yes','No','Yes','No','No','Yes']\n",
    "data = zip(Stolen, Color, Type, Origin)\n",
    "colnames = ['Stolen','Color','Type','Origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of dictionaries for each class and each feature.\n",
    "For example, a dictionary `data_dict` contains two keys `Yes` and `No`. For each key, it has a dictionary which contains 3 features as keys `Color`, `Type` and `Origin`. For each feature as key, it has a dictionary of its values as keys, and each key corresponds to the number of its occurances as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: data_dict['Yes']['Color']['Red'] = 3 -> data_dict[Class_label][Feature_name][Feature_value] = Number_of_occurence\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_data(dataset, column_names):\n",
    "    '''This function is used to load dataset and transform to the nested dictionary as illustrated above.\n",
    "    Inputs:\n",
    "    1) dataset: Dataset as a list of tuples\n",
    "    2) column_names: Column names of this dataset\n",
    "    \n",
    "    Outputs:\n",
    "    1) data_dictionary: A nested dictionary\n",
    "    2) prior_class: A dictionary of frequencies for each class label\n",
    "    3) prior_prob: A dictionary of prior probabilities for each class label\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Initialize this dictionary\n",
    "    data_dictionary = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0)))\n",
    "    prior_class = defaultdict(lambda: 0)\n",
    "    prior_prob = defaultdict(lambda: 0)\n",
    "    \n",
    "    # Number of rows\n",
    "    nrow = len(dataset)\n",
    "\n",
    "    for _, row in enumerate(dataset):\n",
    "        \n",
    "        # Update priors for class labels\n",
    "        prior_class[row[0]] += 1\n",
    "        \n",
    "        # Update prior probabilities for class labels\n",
    "        prior_prob[row[0]] += 1/nrow\n",
    "        \n",
    "        # Update nested dictionary for feature values of each class label\n",
    "        for j in range(1, len(row)):\n",
    "            data_dictionary[row[0]][column_names[j]][row[j]] += 1\n",
    "            \n",
    "    return (data_dictionary, prior_class, prior_prob)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict, prior, prior_probability = load_data(data, colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_dictionary, prior_class, prior_prob, new_dataset, new_data_colnames):\n",
    "    '''This function is used to calculate posterior probability of which class label this new_data belongs to.\n",
    "    \n",
    "    Inputs:\n",
    "    1) data_dictionary: A nested dictionary\n",
    "    2) prior_class: A dictionary for class labels\n",
    "    3) prior_prob: A dictionary of prior probabilities for each class label\n",
    "    4) new_dataset: A tuple of features\n",
    "    5) new_data_colnames: Column names of this new_data\n",
    "    \n",
    "    Outputs:\n",
    "    1) posterior: A dictionary of posterior probabilities of each class label   \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Initialize the dictionary of posterior probabilities: Using prior probabilities\n",
    "    posterior = prior_prob    \n",
    "    \n",
    "    # Multiplying priors with likelihood values\n",
    "    for key in posterior.keys():\n",
    "        for k in range(len(new_dataset)):\n",
    "            # Calculate conditional probability term of this feature\n",
    "            cond_pr = data_dictionary[key][new_data_colnames[k]][new_dataset[k]] / prior_class[key]\n",
    "            \n",
    "            # Multiply\n",
    "            posterior[key] *= cond_pr\n",
    "\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a test\n",
    "# Remember we aim to predict whether a (Red, Domestic, SUV) car will be stolen or not. \n",
    "new_data = ['Red','SUV','Domestic']\n",
    "new_column_names = ['Color','Type','Origin']\n",
    "predict(data_dict, prior, prior_probability, new_data, new_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the posterior probability for 'No' is larger than 'Yes', we can conclude that this car will not be stolen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. How to calculate posterior probability if $P(x_{j}|C_{k})=0$? <br/>\n",
    "Answer: [Laplace smoothing](http://classes.engr.oregonstate.edu/eecs/winter2011/cs434/notes/bayes-6.pdf) <br/>\n",
    "2. What if $x_{j}$ is a continuous variable rather than a categorical variable? <br/>\n",
    "Answer: Assume Normal/Gaussian distribution <br/>\n",
    "3. How to overcome float-point underflow/overflow issue when you calculate data likelihood $\\prod_{j=1}^{f}P(x_{j}|C_{k})$? <br/>\n",
    "Answer: Take logarithmic form and then transform back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Try Open-Source Package\n",
    "### 2.1 Scikit-Learn\n",
    "The package `scikit-learn` can be found at http://scikit-learn.org/stable/index.html. <br/>\n",
    "Please install the package first. <br/>\n",
    "The introduction of Naive Bayes function can be found [here](http://scikit-learn.org/stable/modules/naive_bayes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try a Gaussian Naive Bayes Method\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "clf = GaussianNB().fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we want to know which class does [-0.8, -1] belongs to\n",
    "new_data = np.array([[-0.8, -1]])\n",
    "print clf.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior probabilities for each class label\n",
    "print clf.classes_\n",
    "print clf.predict_proba(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about Multiple Linear Regression can be found at http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Summary of Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "* Easy to implement\n",
    "* Good results obtained in most of the cases\n",
    "\n",
    "### Disadvantages\n",
    "* Assumption: class conditional independence , therefore loss of accuracy\n",
    "* Practically, dependencies exist among variables\n",
    "* Dependencies among these cannot be modeled by Na√Øve Bayesian Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
